{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13b6068",
   "metadata": {},
   "source": [
    "### 0- Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002c2334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_11664\\1961817053.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Result 1\n",
      "One\tof\tthe\tsimplest\tways\tto\tfill\tin\tmissing\tvalues\tis\tto\tcarry\tforward\tthe\tlast\tknown\tvalue\tprior\n",
      "to\n",
      "Metadata: {'page': 37, 'moddate': '2020-03-30T07:09:46+00:00', 'producer': 'PDF Candy', 'page_label': '38', 'total_pages': 365, 'creationdate': '2020-03-30T07:09:46+00:00', 'creator': 'PyPDF', 'source': 'data/ps.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Loading a pre-trained embedding model and initializing a Chroma vector store\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},  # or 'cuda' if available\n",
    "    encode_kwargs={'normalize_embeddings': True}  # for cosine search\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    persist_directory=\"chroma_db\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "query = \"How does the model deal with missing values?\"\n",
    "results = chroma_db.similarity_search(query, k=1)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nüîé Result {i+1}\")\n",
    "    print(doc.page_content[:100])  # Preview first 300 chars\n",
    "    print(\"Metadata:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2c135",
   "metadata": {},
   "source": [
    "## 1- Query translation/optimization:\n",
    "In order to improve the quality and relevance of the results, we do query translation.\n",
    "- Multi-Query: Multiple queries are made from a single query using llm; then, we retrieve the relevant results and combine them\n",
    "- HyDe: Hypothetical document: We create a document from the query and retrieve the most relevants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccfea0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807bbe03d087408f8db6176d71cae0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\projects\\gr\\.denv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rigia\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dc46b77b8b459d954a84fd548f915d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24a8c1442bb4d4b90e058e86898c77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3fd490308940f3821ebd748325067f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36b3b77911e4b07b3a06ebe89ba07ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf3a69e1dd14d1c8cb5f2d34ad029a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b8ce549a6f4673973c6af1ae8362e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is based on a number of factors, such as the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number, the number of digits in a number,\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "def generate_long_answer(query: str, prompt: str = \"Expand this into a comprehensive answer:\", max_tokens: int = 300) -> str:\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    text_gen_pipeline = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
    "\n",
    "    # Combine prompt and query\n",
    "    full_prompt = f\"{prompt} {query}\"\n",
    "    response = llm.invoke(full_prompt)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"How does the model deal with missing values?\"\n",
    "prompt = \"Rewrite and expand this into a detailed technical question:\"\n",
    "print(generate_long_answer(query, prompt=prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7580f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 37, 'producer': 'PDF Candy', 'page_label': '38', 'moddate': '2020-03-30T07:09:46+00:00', 'creationdate': '2020-03-30T07:09:46+00:00', 'creator': 'PyPDF', 'source': 'data/ps.pdf', 'total_pages': 365}, page_content='One\\tof\\tthe\\tsimplest\\tways\\tto\\tfill\\tin\\tmissing\\tvalues\\tis\\tto\\tcarry\\tforward\\tthe\\tlast\\tknown\\tvalue\\tprior\\nto\\tthe\\tmissing\\tone,\\tan\\tapproach\\tknown\\tas\\t\\nforward\\tfill\\n.\\tNo\\tmathematics\\tor\\tcomplicated\\tlogic\\tis\\nrequired.\\tSimply\\tconsider\\tthe\\texperience\\tof\\tmoving\\tforward\\tin\\ttime\\twith\\tthe\\tdata\\tthat\\twas\\navailable,\\tand\\tyou\\tcan\\tsee\\tthat\\tat\\ta\\tmissing\\tpoint\\tin\\ttime,\\tall\\tyou\\tcan\\tbe\\tconfident\\tof\\tis\\twhat\\ndata\\thas\\talready\\tbeen\\trecorded.\\tIn\\tsuch\\ta\\tcase,\\tit\\tmakes\\tsense\\tto\\tuse\\tthe\\tmost\\trecent\\tknown\\nmeasurement.\\nForward\\tfill\\tcan\\tbe\\taccomplished\\teasily\\tusing\\t\\n\\tfrom\\tthe\\t\\n\\tpackage:\\nThis\\twill\\tresult\\tin\\ta\\tplot\\tthat\\tlooks\\tnatural\\texcept\\twhere\\tyou\\tsee\\trepeated\\tvalues\\tto\\taccount\\tfor\\nmissing\\tdata,\\tas\\tin\\t\\nFigure\\t2-6\\n.\\tAs\\tyou\\twill\\tnotice\\tin\\tthe\\tplot,\\tthe\\tforward-filled\\tvalues\\tusually\\tdo\\nnot\\tdeviate\\tfar\\tfrom\\tthe\\ttrue\\tvalues.\\nFigure\\t2-6.\\t\\nThe\\toriginal\\ttime\\tseries\\tplotted\\twith\\ta\\tsolid\\tline\\tand\\tthe\\ttime\\tseries\\twith\\tforward\\tfilled\\tvalues\\tfor\\nrandomly\\tmissing\\tpoints\\tplotted\\twith\\ta\\tdashed\\tline.\\tThe\\tforward\\tfilled\\tvalues\\tare\\tmarked\\twith\\tdownward\\npointing\\ttriangles.\\nWe\\tcan\\talso\\tcompare\\tthe\\tvalues\\tin\\tthe\\tseries\\tby\\tplotting\\tthe\\tvalues\\tof\\tthe\\tseries\\tagainst\\tone\\nanother.\\tThat\\tis,\\tfor\\teach\\ttime\\tstep,\\twe\\tplot\\tthe\\ttrue\\tknown\\tvalue\\tagainst\\tthe\\tvalue\\tat\\tthe\\tsame\\ntime\\tfrom\\tthe\\tseries\\twith\\timputed\\tvalues.\\tMost\\tvalues\\tshould\\tmatch\\texactly\\tsince\\tmost\\tdata\\tis\\npresent.\\tWe\\tsee\\tthat\\tmanifested\\tin\\tthe\\t1:1\\tline\\tin\\t\\nFigure\\t2-7\\n.\\tWe\\talso\\tsee\\tscattered\\tpoints\\toff\\nthis\\tline,\\tbut\\tthey\\tdo\\tnot\\tappear\\tto\\tbe\\tsystematically\\toff.'),\n",
       " Document(metadata={'creationdate': '2020-03-30T07:09:46+00:00', 'page': 251, 'moddate': '2020-03-30T07:09:46+00:00', 'producer': 'PDF Candy', 'source': 'data/ps.pdf', 'creator': 'PyPDF', 'page_label': '252', 'total_pages': 365}, page_content='We\\tsee\\tthe\\tresulting\\tdistribution\\tin\\t\\nFigure\\t11-3\\n.\\tAs\\twe\\tcan\\tsee,\\tthe\\tdistribution\\tis\\tnot\\tas\\tsmooth\\nand\\twell\\tdefined\\tfor\\tthis\\tmisspecified\\tmodel\\tas\\tit\\twas\\tfor\\tthe\\tappropriately\\tspecified\\tmodel\\tfrom\\nFigure\\t11-2\\n.\\nFigure\\t11-3.\\t\\nThe\\testimated\\tdistribution\\tof\\tthe\\tlag\\t1\\tcoefficient\\tfor\\tan\\tAR(1)\\tmodel\\tfitted\\tto\\ta\\tprocess\\tfor\\nwhich\\tthe\\ttrue\\tdescription\\tis\\tan\\tAR(2)\\tprocess.\\nIt\\tmay\\tstrike\\tyou\\tthat\\tthese\\tdistributions\\tare\\tnot\\tall\\tthat\\tdifferent,\\tand\\tyou\\tare\\tright.\\tWe\\nconfirm\\tthis\\twith\\tthe\\tsummary\\tstatistics:\\nWe\\tcan\\tsee\\tthat\\tthe\\trange\\tof\\testimates\\tis\\twider\\twhen\\tthe\\tmodel\\tis\\tmisspecified\\tand\\tthat\\tthe\\nestimate\\tfor\\tthe\\tfirst\\torder\\tterm\\tis\\tslightly\\tworse\\tthan\\tit\\twas\\twhen\\tthe\\tmodel\\twas\\tproperly\\nspecified,\\tbut\\tthe\\tdeviation\\tis\\tnot\\ttoo\\tlarge.\\nThis\\tcan\\taddress\\tconcerns\\tthat\\tunderestimating\\tthe\\torder\\tof\\tthe\\tmodel\\twill\\taffect\\tour\\testimate\\nof\\t\\nœï\\n.\\tWe\\tcan\\trun\\ta\\tvariety\\tof\\tsimulation\\tscenarios\\tto\\taddress\\tpotential\\tissues\\tand\\tunderstand\\nthe\\trange\\tof\\tlikely\\tmisestimation\\tgiven\\tsome\\timagined\\tpossibilities.\\n5'),\n",
       " Document(metadata={'creator': 'PyPDF', 'moddate': '2020-03-30T07:09:46+00:00', 'page': 53, 'source': 'data/ps.pdf', 'creationdate': '2020-03-30T07:09:46+00:00', 'page_label': '54', 'producer': 'PDF Candy', 'total_pages': 365}, page_content='Unfortunately,\\tthere\\tisn‚Äôt\\ta\\tdefinitive\\tstatistical\\tdiagnosis\\tfor\\tlookahead‚Äîafter\\tall,\\tthe\\twhole\\nendeavor\\tof\\ttime\\tseries\\tanalysis\\tis\\tmodeling\\tthe\\tunknown.\\tUnless\\ta\\tsystem\\tis\\tsomewhat\\ndeterministic\\twith\\tknown\\tdynamical\\tlaws,\\tit\\tcan\\tbe\\tdifficult\\tto\\tdistinguish\\ta\\tvery\\tgood\\tmodel\\nfrom\\ta\\tmodel\\twith\\tlookahead‚Äîthat\\tis,\\tuntil\\tyou\\tput\\ta\\tmodel\\tinto\\tproduction\\tand\\trealize\\teither\\nthat\\tyou\\tare\\tmissing\\tdata\\twhen\\tyou\\tplanned\\tto\\thave\\tit,\\tor\\tsimply\\tthat\\tyour\\tresults\\tin\\tproduction\\ndo\\tnot\\treflect\\twhat\\tyou\\tsee\\tduring\\ttraining.\\nThe\\tbest\\tway\\tto\\tprevent\\tthis\\tembarrassment\\tis\\tconstant\\tvigilance.\\tWhenever\\tyou\\tare\\ttime-\\nshifting\\tdata,\\tsmoothing\\tdata,\\timputing\\tdata,\\tor\\tupsampling\\tdata,\\task\\tyourself\\twhether\\tyou\\ncould\\tknow\\tsomething\\tat\\ta\\tgiven\\ttime.\\tRemember\\tthat\\tdoesn‚Äôt\\tjust\\tinclude\\tcalendar\\ttime.\\tIt\\nalso\\tincludes\\trealistic\\ttime\\tlags\\tto\\treflect\\thow\\tlong\\ta\\tdelay\\tthere\\tis\\tbetween\\tsomething\\nhappening\\tand\\tyour\\torganization\\thaving\\tthat\\tdata\\tavailable.\\tFor\\texample,\\tif\\tyour\\torganization\\nscrapes\\tTwitter\\tonly\\tweekly\\tto\\tgather\\tits\\tsentiment\\tanalysis\\tdata,\\tyou\\tneed\\tto\\tinclude\\tthis\\nweekly\\tperiodicity\\tin\\tyour\\ttraining\\tand\\tvalidation\\tdata\\tsegmentation.\\tSimilarly,\\tif\\tyou\\tcan\\nretrain\\tyour\\tmodel\\tonly\\tonce\\ta\\tmonth,\\tyou\\tneed\\tto\\tfigure\\tout\\twhat\\tmodel\\twould\\tapply\\tto\\twhat\\ndata\\tover\\ttime.\\tYou\\tcan‚Äôt,\\tfor\\texample,\\ttrain\\ta\\tmodel\\tfor\\tJuly\\tand\\tthen\\tapply\\tit\\tto\\tJuly\\tfor\\ttesting,\\nbecause\\tin\\ta\\treal\\tsituation,\\tyou\\twouldn‚Äôt\\thave\\tthat\\tmodel\\ttrained\\tup\\tin\\ttime\\tif\\ttraining\\ttakes\\tyou\\na\\tlong\\ttime.\\nHere\\tare\\tsome\\tother\\tideas\\tto\\tuse\\tas\\ta\\tgeneral\\tchecklist.\\tKeep\\tthem\\tin\\tmind\\tboth\\twhen\\tplanning\\nto\\tbuild\\ta\\tmodel\\tand\\twhen\\tauditing\\tyour\\tprocess\\tafter\\tthe\\tfact:\\nIf\\tyou\\tare\\tsmoothing\\tdata\\tor\\timputing\\tmissing\\tdata,\\tthink\\tcarefully\\tabout\\twhether\\tit\\nmight\\timpact\\tyour\\tresults\\tby\\tintroducing\\ta\\tlookahead.\\tAnd\\tdon‚Äôt\\tjust\\tthink\\tabout\\tit‚Äî\\nexperiment\\tas\\twe\\tdid\\tearlier\\tand\\tsee\\thow\\tthe\\timputations\\tand\\tsmoothing\\twork.\\tDo\\tthey\\nseem\\tto\\tbe\\tforward\\tlooking?\\tIf\\tso,\\tcan\\tyou\\tjustify\\tusing\\tthem?\\t(Probably\\tnot.)\\nBuild\\tyour\\tentire\\tprocess\\twith\\ta\\tvery\\tsmall\\tdata\\tset\\t(only\\ta\\tfew\\trows\\tin\\ta\\t\\n\\tor\\ta\\nfew\\trow\\ttime\\tsteps\\tin\\twhatever\\tdata\\tformat).\\tThen,\\tdo\\trandom\\tspot\\tchecks\\tat\\teach\\tstep\\nin\\tthe\\tprocess\\tand\\tsee\\twhether\\tyou\\taccidentally\\tshift\\tany\\tinformation\\ttemporally\\tto\\tan\\ninappropriate\\tplace.\\nFor\\teach\\tkind\\tof')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_db.max_marginal_relevance_search(query, k=3, fetch_k=10, lambda_mult=0.5) # lambda_mult: trade-off between relevance (1.0) and diversity (0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8d48f",
   "metadata": {},
   "source": [
    "**Loading LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43b6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"google/flan-t5-small\"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30023efe",
   "metadata": {},
   "source": [
    "Using it in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df9eb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: How does the model deal with missing values?\n",
      "ü§ñ Answer: By using the most recent known measurement.\n",
      "\n",
      "‚ùì Question: what is lag feature.\n",
      "ü§ñ Answer: The function , which is a proxy for calculating the expected value of L 2 ( X 2 )  L ( X )  X ( L is the lag operator).\n",
      "\n",
      "‚ùì Question: What feature is the most useful features for time series forecasting?\n",
      "ü§ñ Answer: Feature generation\n",
      "[{'query': 'How does the model deal with missing values?', 'result': 'By using the most recent known measurement.'}, {'query': 'what is lag feature.', 'result': 'The function , which is a proxy for calculating the expected value of L 2 ( X 2 )  L ( X )  X ( L is the lag operator).'}, {'query': 'What feature is the most useful features for time series forecasting?', 'result': 'Feature generation'}]\n"
     ]
    }
   ],
   "source": [
    "retriever = chroma_db.as_retriever(search_type=\"mmr\", #search_type can be ‚Äúsimilarity‚Äù, ‚Äúmmr‚Äù, or ‚Äúsimilarity_score_threshold‚Äù\n",
    "                                   search_kwargs={\n",
    "                                        \"k\": 2,\n",
    "                                        \"fetch_k\": 4,\n",
    "                                        \"lambda_mult\": 0.5\n",
    "                                    })\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"How does the model deal with missing values?\",\n",
    "    \"what is lag feature.\",\n",
    "    \"What feature is the most useful features for time series forecasting?\",\n",
    "]\n",
    "\n",
    "qa_results = []\n",
    "for question in questions:\n",
    "    qa = qa_chain.invoke(question)\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(f\"ü§ñ Answer: {qa['result']}\")\n",
    "    qa_results.append(qa)\n",
    "\n",
    "print(qa_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a580526",
   "metadata": {},
   "source": [
    "| Method           | Type              | Requires Ground Truth | Measures                      | Explanation                                                                                  | Common Use Cases                                       |\n",
    "|------------------|-------------------|------------------------|-------------------------------|----------------------------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Precision@k      | Retrieval Metric   | ‚úÖ Yes                 | Accuracy of top-k results     | Fraction of top-k retrieved documents that are relevant.                                     | Evaluate quality of retrieved documents for QA.        |\n",
    "| Recall@k         | Retrieval Metric   | ‚úÖ Yes                 | Coverage of relevant results  | Fraction of all relevant documents that are present in the top-k results.                   | Ensure important documents are not missed in retrieval.|\n",
    "| F1@k             | Retrieval Metric   | ‚úÖ Yes                 | Balance of precision & recall | Harmonic mean of precision and recall, balancing quality and coverage.                      | Optimize both completeness and accuracy of retrieval.  |\n",
    "| MRR (Reciprocal Rank) | Retrieval Metric | ‚úÖ Yes            | Ranking quality               | Average of the reciprocal ranks of the first relevant doc across queries.                   | Measures how early a relevant document appears.        |\n",
    "| RAGAS            | RAG Eval Framework | ‚úÖ Yes (Answers + Contexts preferred) | End-to-end RAG quality     | Evaluates the relevance of context, answer correctness, faithfulness to source, etc.        | QA system evaluation, LLM-based retrieval pipelines.   |\n",
    "\n",
    "\n",
    "**Methods without ground truth**\n",
    "\n",
    "| Method                    | What It Evaluates         | Ground Truth Needed | Description                                                                                   | Typical Use Cases                                      |\n",
    "|---------------------------|---------------------------|---------------------|-----------------------------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Embedding Similarity      | Query-context relevance    | ‚ùå No               | Measures cosine similarity between query and retrieved document embeddings                    | Sanity check for vector retrieval results              |\n",
    "| Context Overlap           | Surface-level relevance    | ‚ùå No               | Measures lexical or semantic overlap between query and retrieved text                         | Quick validation of retrieval                         |\n",
    "| MMR Diversity             | Retrieval diversity        | ‚ùå No               | Evaluates how different the retrieved documents are from each other                           | Ensure non-redundant context in RAG                   |\n",
    "| RAGAS (partial)           | Context quality            | ‚ö†Ô∏è Only Query + Context | RAGAS metrics like context precision can run without answer or gold label                     | Evaluate retrieval without needing answers             |\n",
    "| LLM-as-a-Judge            | Answer/context quality     | ‚ùå No               | Ask an LLM (local or remote) to evaluate relevance, fluency, or factual consistency           | Quick feedback during RAG development                 |\n",
    "| Perplexity / LM Loss      | Answer fluency             | ‚ùå No               | Use the language model's perplexity to estimate how fluent or confident it is in its response | Compare generations across models                     |\n",
    "| Chunk Keyword Match       | Basic context relevance    | ‚ùå No               | Check if retrieved chunks contain key terms from the query                                    | Heuristic filter for retrieval quality                |\n",
    "| Anomaly Detection         | Retrieval reliability      | ‚ùå No               | Flags chunks that are statistically dissimilar or inconsistent with typical results           | Outlier detection in retrieved context                |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**RAGAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392e6d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Dataset' from 'ragas' (c:\\code\\projects\\gr\\.denv\\Lib\\site-packages\\ragas\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate, Dataset\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m faithfulness, answer_relevancy, context_precision, context_recall\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Dataset' from 'ragas' (c:\\code\\projects\\gr\\.denv\\Lib\\site-packages\\ragas\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from langchain_core.documents import Document\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚úÖ RAGAS imports successful\")\n",
    "\n",
    "# Prepare evaluation data\n",
    "eval_data = []\n",
    "for qa in qa_results:\n",
    "    question = qa[\"query\"]\n",
    "    answer = qa[\"result\"]\n",
    "\n",
    "    # Retrieve relevant context docs\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Format for RAGAS\n",
    "    eval_data.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": [Document(page_content=doc.page_content) for doc in retrieved_docs]\n",
    "    })\n",
    "\n",
    "# Convert eval_data to Dataset\n",
    "eval_dataset = Dataset.from_pandas(pd.DataFrame(eval_data))\n",
    "\n",
    "# Run RAGAS evaluation\n",
    "# Run RAGAS evaluation with custom LLM + embedding\n",
    "results = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "    llm=llm,\n",
    "    embeddings=embedding_model\n",
    ")\n",
    "# Print results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b88c146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 3 evaluation examples\n",
      "Example 1: How does the model deal with missing values?...\n",
      "Example 2: what is lag feature....\n",
      "Example 3: What feature is the most useful features for time ...\n"
     ]
    }
   ],
   "source": [
    "# Create evaluation examples for RAGAS\n",
    "class EvaluationExample:\n",
    "    def __init__(self, question, answer, contexts, ground_truth=None):\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.contexts = contexts\n",
    "        self.ground_truth = ground_truth\n",
    "\n",
    "# Create some evaluation examples using our existing QA results\n",
    "evaluation_examples = []\n",
    "\n",
    "# Use our existing qa_results if available, otherwise create some examples\n",
    "if 'qa_results' in locals() and qa_results:\n",
    "    for qa in qa_results[:3]:  # Use first 3 results\n",
    "        # Get contexts from the retrieved documents\n",
    "        contexts = [doc.page_content for doc in qa.get('source_documents', [])]\n",
    "        \n",
    "        example = EvaluationExample(\n",
    "            question=qa['query'],\n",
    "            answer=qa['result'],\n",
    "            contexts=contexts,\n",
    "            ground_truth=qa['result']  # Using generated answer as ground truth for demo\n",
    "        )\n",
    "        evaluation_examples.append(example)\n",
    "else:\n",
    "    # Create some example data for testing\n",
    "    sample_questions = [\n",
    "        \"What is the main purpose of the document?\",\n",
    "        \"What are the key findings?\",\n",
    "        \"What methodology was used?\"\n",
    "    ]\n",
    "    \n",
    "    for question in sample_questions:\n",
    "        # Get answer from QA chain\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        # Get contexts from retriever\n",
    "        retrieved_docs = retriever.get_relevant_documents(question)\n",
    "        contexts = [doc.page_content for doc in retrieved_docs]\n",
    "        \n",
    "        example = EvaluationExample(\n",
    "            question=question,\n",
    "            answer=result['result'],\n",
    "            contexts=contexts,\n",
    "            ground_truth=result['result']\n",
    "        )\n",
    "        evaluation_examples.append(example)\n",
    "\n",
    "print(f\"‚úÖ Created {len(evaluation_examples)} evaluation examples\")\n",
    "for i, example in enumerate(evaluation_examples):\n",
    "    print(f\"Example {i+1}: {example.question[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a0ea4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS evaluation failed: 'DataFrame' object has no attribute 'get_sample_type'\n",
      "Continuing with other evaluation methods...\n"
     ]
    }
   ],
   "source": [
    "# Simple RAGAS evaluation without complex wrappers\n",
    "try:\n",
    "    from ragas import evaluate\n",
    "    from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Prepare evaluation data as a simple list of dictionaries\n",
    "    eval_data = []\n",
    "    for qa in qa_results:\n",
    "        question = qa[\"query\"]\n",
    "        answer = qa[\"result\"]\n",
    "        retrieved_docs = retriever.get_relevant_documents(question)\n",
    "        \n",
    "        eval_data.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"contexts\": [doc.page_content for doc in retrieved_docs]\n",
    "        })\n",
    "    \n",
    "    # Convert to pandas DataFrame (works with most RAGAS versions)\n",
    "    df = pd.DataFrame(eval_data)\n",
    "    \n",
    "    # Run evaluation with minimal configuration\n",
    "    results = evaluate(\n",
    "        df,\n",
    "        metrics=[context_precision, context_recall]  # Use only metrics that don't require LLM\n",
    "    )\n",
    "    \n",
    "    print(\"RAGAS Evaluation Results:\")\n",
    "    print(results)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"RAGAS evaluation failed: {e}\")\n",
    "    print(\"Continuing with other evaluation methods...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "085d8059",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Evaluate embedding similarities for our examples\u001b[39;00m\n\u001b[32m     25\u001b[39m embedding_results = []\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[43mevaluation_examples\u001b[49m:\n\u001b[32m     27\u001b[39m     result = evaluate_embedding_similarity(\n\u001b[32m     28\u001b[39m         example.question, \n\u001b[32m     29\u001b[39m         example.contexts, \n\u001b[32m     30\u001b[39m         embedding_model\n\u001b[32m     31\u001b[39m     )\n\u001b[32m     32\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m] = example.question\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluation_examples' is not defined"
     ]
    }
   ],
   "source": [
    "# Method 1: Embedding Similarity Evaluation\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_embedding_similarity(query, contexts, embedding_model):\n",
    "    \"\"\"Evaluate retrieval quality using embedding similarity\"\"\"\n",
    "    # Encode query and contexts\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    context_embeddings = embedding_model.embed_documents(contexts)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for ctx_emb in context_embeddings:\n",
    "        sim = cosine_similarity([query_embedding], [ctx_emb])[0][0]\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    return {\n",
    "        'mean_similarity': np.mean(similarities),\n",
    "        'max_similarity': np.max(similarities),\n",
    "        'min_similarity': np.min(similarities),\n",
    "        'similarities': similarities\n",
    "    }\n",
    "\n",
    "# Create evaluation examples based on our retrieval results\n",
    "class EvaluationExample:\n",
    "    def __init__(self, question, contexts):\n",
    "        self.question = question\n",
    "        self.contexts = contexts\n",
    "\n",
    "def create_evaluation_examples(questions, retriever, num_contexts=3):\n",
    "    \"\"\"Create evaluation examples from questions and retrieval results\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for question in questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(question)\n",
    "        contexts = [doc.page_content for doc in retrieved_docs[:num_contexts]]\n",
    "        \n",
    "        example = EvaluationExample(\n",
    "            question=question,\n",
    "            contexts=contexts\n",
    "        )\n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Create examples\n",
    "evaluation_examples = create_evaluation_examples(questions, retriever)\n",
    "\n",
    "# Evaluate embedding similarities for our examples\n",
    "embedding_results = []\n",
    "for example in evaluation_examples:\n",
    "    result = evaluate_embedding_similarity(\n",
    "        example.question, \n",
    "        example.contexts, \n",
    "        embedding_model\n",
    "    )\n",
    "    result['question'] = example.question\n",
    "    embedding_results.append(result)\n",
    "\n",
    "print(\"Embedding Similarity Results:\")\n",
    "for result in embedding_results[:3]:  # Show first 3\n",
    "    print(f\"\\nQ: {result['question'][:50]}...\")\n",
    "    print(f\"Mean similarity: {result['mean_similarity']:.3f}\")\n",
    "    print(f\"Max similarity: {result['max_similarity']:.3f}\")\n",
    "    print(f\"Min similarity: {result['min_similarity']:.3f}\")\n",
    "\n",
    "# RAGAS Evaluation with proper dataset format\n",
    "try:\n",
    "    # Prepare data for RAGAS evaluation\n",
    "    ragas_data = {\n",
    "        'question': [],\n",
    "        'answer': [],\n",
    "        'contexts': [],\n",
    "        'ground_truth': []\n",
    "    }\n",
    "    \n",
    "    # Convert evaluation examples to RAGAS format\n",
    "    for example in evaluation_examples:\n",
    "        ragas_data['question'].append(example.question)\n",
    "        ragas_data['answer'].append(example.answer)\n",
    "        ragas_data['contexts'].append(example.contexts)\n",
    "        ragas_data['ground_truth'].append(example.ground_truth or example.answer)\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    ragas_dataset = Dataset.from_dict(ragas_data)\n",
    "    \n",
    "    print(\"‚úÖ Dataset created successfully\")\n",
    "    print(f\"Dataset size: {len(ragas_dataset)}\")\n",
    "    print(\"\\nDataset structure:\")\n",
    "    print(ragas_dataset)\n",
    "    \n",
    "    # Run RAGAS evaluation\n",
    "    print(\"\\nüîÑ Running RAGAS evaluation...\")\n",
    "    \n",
    "    # Use a subset of metrics to avoid timeout\n",
    "    metrics = [faithfulness, answer_relevancy]\n",
    "    \n",
    "    result = evaluate(\n",
    "        dataset=ragas_dataset,\n",
    "        metrics=metrics,\n",
    "        raise_exceptions=False  # Don't fail on individual metric errors\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ RAGAS Evaluation Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Convert results to DataFrame for better display\n",
    "    results_df = result.to_pandas()\n",
    "    print(results_df[['question', 'faithfulness', 'answer_relevancy']].head())\n",
    "    \n",
    "    print(f\"\\nAverage Scores:\")\n",
    "    for metric in ['faithfulness', 'answer_relevancy']:\n",
    "        if metric in results_df.columns:\n",
    "            avg_score = results_df[metric].mean()\n",
    "            print(f\"  {metric}: {avg_score:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå RAGAS evaluation failed: {str(e)}\")\n",
    "    print(\"\\nüîÑ Falling back to custom evaluation methods...\")\n",
    "    \n",
    "    # Fallback to our custom evaluation functions\n",
    "    print(\"\\n1. Embedding Similarity Evaluation:\")\n",
    "    embedding_results = []\n",
    "    for example in evaluation_examples:\n",
    "        result = evaluate_embedding_similarity(\n",
    "            example.question, \n",
    "            example.contexts, \n",
    "            embedding_model\n",
    "        )\n",
    "        result['question'] = example.question\n",
    "        embedding_results.append(result)\n",
    "    \n",
    "    embedding_df = pd.DataFrame(embedding_results)\n",
    "    print(embedding_df[['question', 'avg_similarity', 'max_similarity']].head())\n",
    "    \n",
    "    print(f\"\\nAverage embedding similarity: {embedding_df['avg_similarity'].mean():.3f}\")\n",
    "    \n",
    "    print(\"\\n2. Keyword Overlap Evaluation:\")\n",
    "    keyword_results = []\n",
    "    for example in evaluation_examples:\n",
    "        result = evaluate_keyword_overlap(example.question, example.contexts)\n",
    "        result['question'] = example.question\n",
    "        keyword_results.append(result)\n",
    "    \n",
    "    keyword_df = pd.DataFrame(keyword_results)\n",
    "    print(keyword_df[['question', 'avg_overlap', 'max_overlap']].head())\n",
    "    \n",
    "    print(f\"\\nAverage keyword overlap: {keyword_df['avg_overlap'].mean():.3f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Custom evaluation completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Keyword Overlap Analysis\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def extract_keywords(text, min_length=3):\n",
    "    \"\"\"Extract meaningful keywords from text\"\"\"\n",
    "    # Convert to lowercase and extract words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Filter out stopwords and short words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [word for word in words if word not in stop_words and len(word) >= min_length]\n",
    "    \n",
    "    return set(keywords)\n",
    "\n",
    "def evaluate_keyword_overlap(query, contexts):\n",
    "    \"\"\"Evaluate retrieval using keyword overlap\"\"\"\n",
    "    query_keywords = extract_keywords(query)\n",
    "    \n",
    "    overlaps = []\n",
    "    for context in contexts:\n",
    "        context_keywords = extract_keywords(context)\n",
    "        \n",
    "        if len(query_keywords) == 0:\n",
    "            overlap = 0.0\n",
    "        else:\n",
    "            intersection = len(query_keywords.intersection(context_keywords))\n",
    "            overlap = intersection / len(query_keywords)\n",
    "        \n",
    "        overlaps.append(overlap)\n",
    "    \n",
    "    return {\n",
    "        'mean_overlap': np.mean(overlaps),\n",
    "        'max_overlap': np.max(overlaps),\n",
    "        'overlaps': overlaps,\n",
    "        'query_keywords': query_keywords\n",
    "    }\n",
    "\n",
    "# Evaluate keyword overlaps\n",
    "keyword_results = []\n",
    "for example in evaluation_examples:\n",
    "    result = evaluate_keyword_overlap(example.question, example.contexts)\n",
    "    result['question'] = example.question\n",
    "    keyword_results.append(result)\n",
    "\n",
    "print(\"Keyword Overlap Results:\")\n",
    "for result in keyword_results[:3]:\n",
    "    print(f\"\\nQ: {result['question'][:50]}...\")\n",
    "    print(f\"Query keywords: {list(result['query_keywords'])[:5]}...\")  # Show first 5\n",
    "    print(f\"Mean overlap: {result['mean_overlap']:.3f}\")\n",
    "    print(f\"Max overlap: {result['max_overlap']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: LLM-as-a-Judge for Context Relevance\n",
    "def evaluate_context_relevance_llm(query, contexts, llm, max_contexts=2):\n",
    "    \"\"\"Use LLM to judge context relevance\"\"\"\n",
    "    relevance_scores = []\n",
    "    \n",
    "    for i, context in enumerate(contexts[:max_contexts]):  # Limit to avoid token limits\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate how relevant the following context is to answering the given question.\n",
    "        Rate the relevance on a scale of 1-5 where:\n",
    "        1 = Not relevant at all\n",
    "        2 = Slightly relevant\n",
    "        3 = Moderately relevant  \n",
    "        4 = Highly relevant\n",
    "        5 = Perfectly relevant\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Context: {context[:500]}...\n",
    "        \n",
    "        Provide only the numerical score (1-5):\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            # Extract numerical score from response\n",
    "            score_match = re.search(r'[1-5]', response['text'] if isinstance(response, dict) else str(response))\n",
    "            score = int(score_match.group()) if score_match else 3  # Default to 3 if parsing fails\n",
    "            relevance_scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating context {i}: {e}\")\n",
    "            relevance_scores.append(3)  # Default score\n",
    "    \n",
    "    return {\n",
    "        'mean_relevance': np.mean(relevance_scores),\n",
    "        'relevance_scores': relevance_scores\n",
    "    }\n",
    "\n",
    "# Evaluate using LLM-as-a-Judge (limiting to first 2 examples due to processing time)\n",
    "print(\"LLM-as-a-Judge Context Relevance Evaluation:\")\n",
    "llm_results = []\n",
    "\n",
    "for i, example in enumerate(evaluation_examples[:2]):  # Limit for demo\n",
    "    print(f\"\\nEvaluating example {i+1}...\")\n",
    "    result = evaluate_context_relevance_llm(example.question, example.contexts, llm)\n",
    "    result['question'] = example.question\n",
    "    llm_results.append(result)\n",
    "    \n",
    "    print(f\"Q: {result['question'][:50]}...\")\n",
    "    print(f\"Mean relevance score: {result['mean_relevance']:.2f}/5\")\n",
    "    print(f\"Individual scores: {result['relevance_scores']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b78760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Context Diversity Evaluation\n",
    "def evaluate_context_diversity(contexts, embedding_model):\n",
    "    \"\"\"Evaluate how diverse the retrieved contexts are\"\"\"\n",
    "    if len(contexts) < 2:\n",
    "        return {'diversity_score': 1.0, 'pairwise_similarities': []}\n",
    "    \n",
    "    # Get embeddings for all contexts\n",
    "    context_embeddings = embedding_model.embed_documents(contexts)\n",
    "    \n",
    "    # Calculate pairwise similarities\n",
    "    pairwise_sims = []\n",
    "    for i in range(len(context_embeddings)):\n",
    "        for j in range(i + 1, len(context_embeddings)):\n",
    "            sim = cosine_similarity([context_embeddings[i]], [context_embeddings[j]])[0][0]\n",
    "            pairwise_sims.append(sim)\n",
    "    \n",
    "    # Diversity score = 1 - average pairwise similarity\n",
    "    avg_similarity = np.mean(pairwise_sims)\n",
    "    diversity_score = 1 - avg_similarity\n",
    "    \n",
    "    return {\n",
    "        'diversity_score': diversity_score,\n",
    "        'avg_pairwise_similarity': avg_similarity,\n",
    "        'pairwise_similarities': pairwise_sims\n",
    "    }\n",
    "\n",
    "# Evaluate diversity for all examples\n",
    "diversity_results = []\n",
    "for example in evaluation_examples:\n",
    "    result = evaluate_context_diversity(example.contexts, embedding_model)\n",
    "    result['question'] = example.question\n",
    "    diversity_results.append(result)\n",
    "\n",
    "print(\"Context Diversity Results:\")\n",
    "for result in diversity_results[:3]:\n",
    "    print(f\"\\nQ: {result['question'][:50]}...\")\n",
    "    print(f\"Diversity score: {result['diversity_score']:.3f} (higher = more diverse)\")\n",
    "    print(f\"Avg pairwise similarity: {result['avg_pairwise_similarity']:.3f}\")\n",
    "\n",
    "# Comprehensive Evaluation Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for i, example in enumerate(evaluation_examples):\n",
    "    summary_data.append({\n",
    "        'Query': example.question[:30] + \"...\",\n",
    "        'Embedding_Sim': embedding_results[i]['mean_similarity'],\n",
    "        'Keyword_Overlap': keyword_results[i]['mean_overlap'],\n",
    "        'Diversity': diversity_results[i]['diversity_score'],\n",
    "        'Num_Contexts': len(example.contexts)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.round(3))\n",
    "\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"Average Embedding Similarity: {summary_df['Embedding_Sim'].mean():.3f}\")\n",
    "print(f\"Average Keyword Overlap: {summary_df['Keyword_Overlap'].mean():.3f}\")\n",
    "print(f\"Average Diversity: {summary_df['Diversity'].mean():.3f}\")\n",
    "\n",
    "# Quality interpretation\n",
    "print(f\"\\nQuality Interpretation:\")\n",
    "avg_emb_sim = summary_df['Embedding_Sim'].mean()\n",
    "if avg_emb_sim > 0.7:\n",
    "    print(\"‚úÖ Excellent embedding similarity - highly relevant contexts\")\n",
    "elif avg_emb_sim > 0.5:\n",
    "    print(\"‚úÖ Good embedding similarity - relevant contexts\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Low embedding similarity - may need better retrieval\")\n",
    "\n",
    "avg_diversity = summary_df['Diversity'].mean()\n",
    "if avg_diversity > 0.3:\n",
    "    print(\"‚úÖ Good diversity - contexts provide varied information\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Low diversity - contexts may be redundant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
