{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dab16b1",
   "metadata": {},
   "source": [
    "# ðŸ“š Complete Testing Guide for Agent007 - From Zero to Hero\n",
    "\n",
    "## ðŸŽ¯ What is Testing and Why Does It Matter?\n",
    "\n",
    "### The Problem\n",
    "Imagine you build a robot that answers customer questions. You change some code to make it faster. But did you accidentally break something? How do you know it still works correctly?\n",
    "\n",
    "**Without tests:** You have to manually ask the robot 100 questions every time you change anything. Exhausting! ðŸ˜°\n",
    "\n",
    "**With tests:** You write the questions once. The computer asks them automatically. In seconds, you know if anything broke. ðŸš€\n",
    "\n",
    "### What is a Test?\n",
    "A test is a piece of code that:\n",
    "1. Asks your system a question\n",
    "2. Checks if the answer is correct\n",
    "3. Tells you \"PASS âœ…\" or \"FAIL âŒ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c71a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: This is a test\n",
    "def test_calculator_adds_correctly():\n",
    "    result = 2 + 3\n",
    "    assert result == 5  # Check if it's correct\n",
    "    print(\"âœ… Test PASSED!\")\n",
    "    # If result is 5 â†’ Test PASSES âœ…\n",
    "    # If result is anything else â†’ Test FAILS âŒ\n",
    "\n",
    "# Let's run it\n",
    "test_calculator_adds_correctly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15540e5",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Testing in Agent007: The Big Picture\n",
    "\n",
    "Agent007 is a system where AI agents answer business questions about customer data. We need to test that:\n",
    "1. **The agents exist and are configured correctly**\n",
    "2. **The agents can understand questions**\n",
    "3. **The agents give sensible answers**\n",
    "4. **The agents don't crash when they make mistakes**\n",
    "\n",
    "### Testing Pyramid (From Fast to Slow)\n",
    "\n",
    "```\n",
    "        /\\\n",
    "       /  \\      â† Few expensive tests\n",
    "      / E  \\       (Run real AI, real database)\n",
    "     /______\\\n",
    "    /        \\    â† Some medium tests\n",
    "   /    I    \\     (Check AI configuration)\n",
    "  /___________\\\n",
    " /             \\  â† Many cheap tests\n",
    "/       U       \\   (Check code syntax, imports)\n",
    "\\_______________/\n",
    "\n",
    "U = Unit Tests (Fast, Free, Many)\n",
    "I = Integration Tests (Medium speed, Some cost)\n",
    "E = Evaluation Tests (Slow, Expensive, Few)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742e55c",
   "metadata": {},
   "source": [
    "## ðŸ“ Where Tests Live in Agent007\n",
    "\n",
    "```\n",
    "agent007/\n",
    "â”œâ”€â”€ tests/                          â† Your test files live here\n",
    "â”‚   â”œâ”€â”€ test_agents.py             â† Tests for root & database agents\n",
    "â”‚   â””â”€â”€ test_churn_agent.py        â† Tests for churn analysis agent\n",
    "â”œâ”€â”€ eval/                           â† Advanced evaluation tests\n",
    "â”‚   â”œâ”€â”€ test_eval.py               â† Runs evaluation scenarios\n",
    "â”‚   â””â”€â”€ eval_data/\n",
    "â”‚       â””â”€â”€ simple.test.json       â† Test scenarios with expected answers\n",
    "â”œâ”€â”€ data_science/                   â† The actual agent code (what we test)\n",
    "â”‚   â”œâ”€â”€ agent.py                   â† Root agent\n",
    "â”‚   â””â”€â”€ sub_agents/\n",
    "â”‚       â”œâ”€â”€ churn/\n",
    "â”‚       â””â”€â”€ bigquery/\n",
    "â””â”€â”€ pyproject.toml                  â† Lists testing tools we need\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2081b",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Testing Tools: What Do They Do?\n",
    "\n",
    "### 1. unittest (Built into Python)\n",
    "- **What it does:** Provides basic testing structure\n",
    "- **Like:** A recipe template for writing tests\n",
    "- **Example:** \"Every test must have `test_` at the start of its name\"\n",
    "\n",
    "### 2. pytest (Advanced Testing Framework)\n",
    "- **What it does:** Runs tests and shows pretty results\n",
    "- **Like:** A smart assistant that finds all your tests and runs them\n",
    "- **Bonus:** Can organize tests with labels (markers)\n",
    "\n",
    "### 3. unittest.IsolatedAsyncioTestCase (Special Type)\n",
    "- **What it does:** Handles tests that wait for things (like AI responses)\n",
    "- **Why we need it:** Talking to AI/databases takes time; normal tests can't wait\n",
    "- **Like:** A patient test runner that knows how to wait\n",
    "\n",
    "### 4. ADK Testing Tools\n",
    "- **Runner:** Executes your agent like a user would\n",
    "- **InMemorySessionService:** Creates fake conversations for testing\n",
    "- **InMemoryArtifactService:** Stores plots/files created during tests\n",
    "- **AgentEvaluator:** Advanced tool that grades agent quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6423b7",
   "metadata": {},
   "source": [
    "## ðŸ“– Anatomy of a Test File\n",
    "\n",
    "Let's break down `test_churn_agent.py` line by line to understand how tests are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SECTION 1: IMPORTS ==============\n",
    "# These bring in the tools we need\n",
    "\n",
    "import unittest                      # Basic testing framework\n",
    "from google.adk.runners import Runner            # Runs the agent\n",
    "from google.adk.sessions import InMemorySessionService  # Fake conversations\n",
    "from google.adk.artifacts import InMemoryArtifactService # Stores outputs\n",
    "from google.genai import types                   # Message formatting\n",
    "\n",
    "# Import the agents we want to test\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from data_science.sub_agents.churn.agent import root_agent as churn_agent\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ddb0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SECTION 2: SETUP SERVICES ==============\n",
    "# Create shared resources (reused across all tests)\n",
    "\n",
    "session_service = InMemorySessionService()    # Manages fake conversations\n",
    "artifact_service = InMemoryArtifactService()  # Stores plots/files\n",
    "\n",
    "print(\"âœ… Services initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SECTION 3 & 4: TEST CLASS WITH SETUP ==============\n",
    "# This is a container for all our tests with setup method\n",
    "\n",
    "class TestChurnAgent(unittest.IsolatedAsyncioTestCase):\n",
    "    \"\"\"Test cases for the churn agent.\"\"\"\n",
    "    # â†‘ This special class lets us test async (waiting) code\n",
    "    \n",
    "    async def asyncSetUp(self):\n",
    "        \"\"\"Runs BEFORE each test (creates a fresh environment).\"\"\"\n",
    "        # Create a fake user session\n",
    "        self.session = await session_service.create_session(\n",
    "            app_name=\"DataAgent\",\n",
    "            user_id=\"test_user\"\n",
    "        )\n",
    "        \n",
    "        # Create the agent runner (like a fake chat interface)\n",
    "        self.runner = Runner(\n",
    "            app_name=\"DataAgent\",\n",
    "            agent=None,  # We'll set this in each test\n",
    "            artifact_service=artifact_service,\n",
    "            session_service=session_service\n",
    "        )\n",
    "        \n",
    "        # Remember user and session IDs for later\n",
    "        self.user_id = \"test_user\"\n",
    "        self.session_id = self.session.id\n",
    "\n",
    "print(\"âœ… Test class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba36ae",
   "metadata": {},
   "source": [
    "### Helper Method: Running Agents\n",
    "\n",
    "This is a reusable function that we'll use in every test to ask the agent questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73173bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SECTION 5: HELPER METHOD ==============\n",
    "# Add this to the TestChurnAgent class\n",
    "\n",
    "def _run_agent(self, agent, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Asks an agent a question and returns the answer.\n",
    "    \n",
    "    Args:\n",
    "        agent: Which agent to ask (churn_agent, root_agent, etc.)\n",
    "        query: The question as text\n",
    "        \n",
    "    Returns:\n",
    "        The agent's text response\n",
    "    \"\"\"\n",
    "    # Tell the runner which agent to use\n",
    "    self.runner.agent = agent\n",
    "    \n",
    "    # Format the question in ADK's format\n",
    "    content = types.Content(\n",
    "        role=\"user\",  # We're the user\n",
    "        parts=[types.Part(text=query)]  # Our question\n",
    "    )\n",
    "    \n",
    "    # Run the agent and collect all events\n",
    "    events = list(self.runner.run(\n",
    "        user_id=self.user_id,\n",
    "        session_id=self.session_id,\n",
    "        new_message=content\n",
    "    ))\n",
    "    \n",
    "    # Get the last event (final response)\n",
    "    last_event = events[-1]\n",
    "    \n",
    "    # Extract text from the response\n",
    "    response = \"\".join(\n",
    "        part.text \n",
    "        for part in last_event.content.parts \n",
    "        if part.text\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Add this method to our test class\n",
    "TestChurnAgent._run_agent = _run_agent\n",
    "\n",
    "print(\"âœ… Helper method added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41a24e",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Understanding Test Methods (The Checks)\n",
    "\n",
    "Now let's write actual tests! Each test method checks one specific thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c04460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST #1: Check if churn agent has correct instructions (CHEAP - no AI calls)\n",
    "async def test_churn_agent_prompt_contains_rules():\n",
    "    \"\"\"\n",
    "    TEST #1: Check if churn agent has correct instructions\n",
    "    This is CHEAP - doesn't call AI or database\n",
    "    \"\"\"\n",
    "    # Get the agent's instruction text\n",
    "    prompt = churn_agent.instruction\n",
    "    \n",
    "    # Check if important phrases exist\n",
    "    assert \"HIGH CHURN RISK\" in prompt\n",
    "    assert \"GROWING CUSTOMER\" in prompt\n",
    "    assert \"NEW CUSTOMER\" in prompt\n",
    "    print(\"âœ… TEST #1 PASSED: Prompt contains all required classification rules\")\n",
    "\n",
    "# Run the test\n",
    "await test_churn_agent_prompt_contains_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST #2: Check if agent has correct name (CHEAP)\n",
    "async def test_churn_agent_has_correct_name():\n",
    "    \"\"\"\n",
    "    TEST #2: Check if agent has the correct name\n",
    "    This is CHEAP - just checks configuration\n",
    "    \"\"\"\n",
    "    assert churn_agent.name == \"churn_agent\"\n",
    "    print(f\"âœ… TEST #2 PASSED: Agent name is '{churn_agent.name}'\")\n",
    "\n",
    "# Run the test\n",
    "await test_churn_agent_has_correct_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc03e7",
   "metadata": {},
   "source": [
    "### Common Test Assertions Reference\n",
    "\n",
    "| Assertion | What It Checks | Example |\n",
    "|-----------|---------------|---------|\n",
    "| `assertEqual(a, b)` | Are a and b exactly the same? | `assertEqual(5, calculator.add(2,3))` |\n",
    "| `assertNotEqual(a, b)` | Are a and b different? | `assertNotEqual(0, len(response))` |\n",
    "| `assertTrue(x)` | Is x true? | `assertTrue(agent.is_ready)` |\n",
    "| `assertFalse(x)` | Is x false? | `assertFalse(agent.has_error)` |\n",
    "| `assertIn(item, container)` | Is item inside container? | `assertIn(\"CHURN\", response)` |\n",
    "| `assertIsNone(x)` | Is x None/null? | `assertIsNone(error)` |\n",
    "| `assertIsNotNone(x)` | Is x NOT None? | `assertIsNotNone(response)` |\n",
    "| `assertGreater(a, b)` | Is a > b? | `assertGreater(len(response), 100)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce591f",
   "metadata": {},
   "source": [
    "## ðŸš€ Running Tests: Different Methods\n",
    "\n",
    "Now that we understand how tests work, let's learn how to run them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73609c",
   "metadata": {},
   "source": [
    "### Method 1: Run ALL Tests (Terminal)\n",
    "\n",
    "```bash\n",
    "# This runs every test in the tests/ folder\n",
    "uv run python -m unittest discover tests/\n",
    "```\n",
    "\n",
    "### Method 2: Run One Test File\n",
    "\n",
    "```bash\n",
    "# Run only churn agent tests\n",
    "uv run python -m unittest tests/test_churn_agent.py\n",
    "```\n",
    "\n",
    "### Method 3: Run One Specific Test\n",
    "\n",
    "```bash\n",
    "# Run just the prompt check test\n",
    "uv run python -m unittest tests.test_churn_agent.TestChurnAgent.test_churn_agent_prompt_contains_rules\n",
    "```\n",
    "\n",
    "### Method 4: Using pytest (Advanced)\n",
    "\n",
    "```bash\n",
    "# Run all tests with prettier output\n",
    "uv run pytest tests/\n",
    "\n",
    "# Run with verbose output (shows test names)\n",
    "uv run pytest tests/ -v\n",
    "\n",
    "# Run and stop at first failure\n",
    "uv run pytest tests/ -x\n",
    "\n",
    "# Run only tests marked with @pytest.mark.db_agent\n",
    "uv run pytest tests/ -m db_agent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac2691",
   "metadata": {},
   "source": [
    "## ðŸ” Reading Test Results\n",
    "\n",
    "### Scenario 1: All Tests Pass âœ…\n",
    "\n",
    "```\n",
    "test_churn_agent_prompt_contains_rules ... ok\n",
    "test_root_agent_has_churn_sub_agent ... ok\n",
    "test_churn_basic_query ... ok\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 3 tests in 2.451s\n",
    "\n",
    "OK\n",
    "```\n",
    "\n",
    "**Meaning:** Everything works! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0726d",
   "metadata": {},
   "source": [
    "### Scenario 2: Test Fails âŒ\n",
    "\n",
    "```\n",
    "test_churn_agent_prompt_contains_rules ... FAIL\n",
    "\n",
    "======================================================================\n",
    "FAIL: test_churn_agent_prompt_contains_rules\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"tests/test_churn_agent.py\", line 50, in test_churn_agent_prompt_contains_rules\n",
    "    self.assertIn(\"HIGH CHURN RISK\", prompt)\n",
    "AssertionError: 'HIGH CHURN RISK' not found in '...'\n",
    "```\n",
    "\n",
    "**How to read this:**\n",
    "1. **Which test failed:** `test_churn_agent_prompt_contains_rules`\n",
    "2. **Which line:** Line 50 in `tests/test_churn_agent.py`\n",
    "3. **What went wrong:** \"HIGH CHURN RISK\" wasn't found in the prompt\n",
    "4. **What to do:** Check if the prompt actually contains that text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd39383",
   "metadata": {},
   "source": [
    "## âœï¸ Writing Your Own Test: Complete Example\n",
    "\n",
    "Let's write a test from scratch to check if the churn agent can identify lost customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699fbba",
   "metadata": {},
   "source": [
    "### Step 1: Plan What to Test\n",
    "- **What:** Churn agent should recognize lost customers\n",
    "- **How:** Ask about lost customers, check if response mentions them\n",
    "- **Why:** Make sure the LOST classification works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e08f4d",
   "metadata": {},
   "source": [
    "### Step 2: Write the Test\n",
    "\n",
    "Here's a complete example of writing your own test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Write your own test for lost customers\n",
    "\n",
    "async def test_churn_agent_understands_lost_concept():\n",
    "    \"\"\"\n",
    "    Verify the churn agent understands the concept of 'LOST' customers.\n",
    "    This is a CHEAP test - just checks the prompt has the right info.\n",
    "    \"\"\"\n",
    "    prompt = churn_agent.instruction\n",
    "    \n",
    "    # Check we got something\n",
    "    assert prompt is not None, \"Agent should have instructions\"\n",
    "    \n",
    "    # Check it's not empty\n",
    "    assert len(prompt) > 100, \"Prompt should be substantial (>100 chars)\"\n",
    "    \n",
    "    # Check it mentions LOST customers\n",
    "    assert \"LOST\" in prompt.upper(), \"Prompt should mention LOST customer classification\"\n",
    "    \n",
    "    # Check it explains what LOST means\n",
    "    assert \"previous\" in prompt.lower() or \"before\" in prompt.lower(), \\\n",
    "        \"Prompt should explain LOST customers have previous revenue\"\n",
    "    \n",
    "    print(\"âœ… TEST PASSED: Agent understands LOST customer concept\")\n",
    "    print(f\"\\nPrompt length: {len(prompt)} characters\")\n",
    "\n",
    "# Run the test\n",
    "await test_churn_agent_understands_lost_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5d7cb",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Test Types by Cost & Speed\n",
    "\n",
    "Understanding which tests to run when:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16979f1",
   "metadata": {},
   "source": [
    "| Test Type | Purpose | Cost | Speed | When to Use |\n",
    "|-----------|---------|------|-------|-------------|\n",
    "| **Unit (Configuration)** | Verify agent setup | Free | < 1 sec | Every commit |\n",
    "| **Unit (Prompt)** | Check instructions | Free | < 1 sec | After changing prompts |\n",
    "| **Integration** | Test real agent | $0.01-0.10 | 5-30 sec | Pre-merge, important features |\n",
    "| **Evaluation** | Regression testing | $1-10 | Minutes | Weekly, before releases |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of each type:\n",
    "\n",
    "# Type 1: Configuration Test (FREE, INSTANT)\n",
    "async def test_type1_configuration():\n",
    "    \"\"\"Check if agent name is correct.\"\"\"\n",
    "    assert churn_agent.name == \"churn_agent\"\n",
    "    print(\"âœ… Type 1: Configuration test (FREE, <0.01s)\")\n",
    "\n",
    "await test_type1_configuration()\n",
    "\n",
    "# Type 2: Prompt Validation (FREE, FAST)\n",
    "async def test_type2_prompt():\n",
    "    \"\"\"Check if instructions contain key phrases.\"\"\"\n",
    "    prompt = churn_agent.instruction\n",
    "    assert \"HIGH CHURN RISK\" in prompt\n",
    "    assert \"churn_metric > 1.5\" in prompt\n",
    "    print(\"âœ… Type 2: Prompt validation test (FREE, <0.01s)\")\n",
    "\n",
    "await test_type2_prompt()\n",
    "\n",
    "# Type 3: Integration Test ($$, SLOW)\n",
    "# NOTE: This would actually call the AI and cost money, so we'll skip it\n",
    "print(\"â­ï¸  Type 3: Integration test (calls real AI, costs ~$0.05, takes 5-30s)\")\n",
    "print(\"   Example: response = _run_agent(churn_agent, 'Show high risk customers')\")\n",
    "\n",
    "# Type 4: Evaluation Test ($$$, VERY SLOW)\n",
    "print(\"â­ï¸  Type 4: Evaluation test (many AI calls, costs $1-10, takes minutes)\")\n",
    "print(\"   Example: AgentEvaluator.evaluate('data_science', 'scenarios.json')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a803f62",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Advanced: The `eval/` Folder - Automated Agent Evaluation\n",
    "\n",
    "The `eval/` folder contains **ADK's Agent Evaluation Framework** - a powerful system for automated quality testing of your agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a84089",
   "metadata": {},
   "source": [
    "### What is Agent Evaluation?\n",
    "\n",
    "**Regular tests** check if your code works: \"Does the function return something?\"\n",
    "\n",
    "**Evaluation tests** check if your agent is **smart**: \n",
    "- \"Does it use the right tools?\"\n",
    "- \"Does it give high-quality answers?\"\n",
    "- \"Is the response accurate and helpful?\"\n",
    "\n",
    "Think of it like this:\n",
    "- **Regular test:** \"Did the student submit an essay?\" âœ…\n",
    "- **Evaluation:** \"Is the essay well-written and accurate?\" ðŸ“ A+\n",
    "\n",
    "### The `eval/` Folder Structure\n",
    "\n",
    "```\n",
    "eval/\n",
    "â”œâ”€â”€ test_eval.py              â† Runs the evaluation tests\n",
    "â””â”€â”€ eval_data/\n",
    "    â”œâ”€â”€ simple.test.json      â† Test scenarios with expected behavior\n",
    "    â””â”€â”€ test_config.json      â† Scoring criteria\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8cc0c",
   "metadata": {},
   "source": [
    "### Understanding `test_eval.py`\n",
    "\n",
    "This file runs the evaluation tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: eval/test_eval.py\n",
    "\n",
    "import os\n",
    "import pytest\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from google.adk.evaluation.agent_evaluator import AgentEvaluator\n",
    "\n",
    "# Enable async testing\n",
    "pytest_plugins = (\"pytest_asyncio\",)\n",
    "\n",
    "# Load environment variables before tests\n",
    "@pytest.fixture(scope=\"session\", autouse=True)\n",
    "def load_env():\n",
    "    load_dotenv(find_dotenv(\".env\"))\n",
    "\n",
    "# The actual evaluation test\n",
    "@pytest.mark.asyncio\n",
    "async def test_eval_simple():\n",
    "    \"\"\"\n",
    "    Test the agent's basic ability via evaluation scenarios.\n",
    "    \n",
    "    This runs ALL scenarios in simple.test.json and grades the agent:\n",
    "    - Did it use the right tools?\n",
    "    - Did it give good answers?\n",
    "    \"\"\"\n",
    "    await AgentEvaluator.evaluate(\n",
    "        \"data_science\",  # Agent module to test\n",
    "        os.path.join(os.path.dirname(__file__), \"eval_data/simple.test.json\"),\n",
    "        num_runs=1,  # Run each scenario once\n",
    "    )\n",
    "\n",
    "print(\"âœ… This is the structure of eval/test_eval.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbaf2a",
   "metadata": {},
   "source": [
    "### Understanding `simple.test.json` - Test Scenarios\n",
    "\n",
    "This file contains test scenarios with expected behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06591d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: eval/eval_data/simple.test.json structure\n",
    "\n",
    "test_scenario = {\n",
    "    # The question to ask the agent\n",
    "    \"query\": \"what countries are in test?\",\n",
    "    \n",
    "    # Which tools should the agent use?\n",
    "    \"expected_tool_use\": [\n",
    "        {\n",
    "            \"tool_name\": \"call_db_agent\",\n",
    "            \"tool_input\": {\n",
    "                \"question\": \"What are the distinct countries in the test table?\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # Expected intermediate responses (optional)\n",
    "    \"expected_intermediate_agent_responses\": [],\n",
    "    \n",
    "    # What the ideal answer looks like (for grading)\n",
    "    \"reference\": \"\"\"**Result:** The distinct countries in the test table are \n",
    "    Canada, Finland, Italy, Kenya, Norway, and Singapore.\n",
    "    \n",
    "    **Explanation:** I queried the `test` table to find all the distinct \n",
    "    values in the `country` column.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"âœ… Each scenario tests one agent interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a42652",
   "metadata": {},
   "source": [
    "### Understanding `test_config.json` - Scoring Criteria\n",
    "\n",
    "This file defines how strictly to grade the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d206c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: eval/eval_data/test_config.json\n",
    "\n",
    "scoring_config = {\n",
    "    \"criteria\": {\n",
    "        # Tool Trajectory: Did agent use the right tools? (1.0 = perfect match required)\n",
    "        \"tool_trajectory_avg_score\": 1.0,\n",
    "        \n",
    "        # Response Match: How similar is answer to reference? (0.1 = lenient, 1.0 = strict)\n",
    "        \"response_match_score\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Explanation:\n",
    "# - tool_trajectory_avg_score: 1.0 means agent MUST use exact tools specified\n",
    "# - response_match_score: 0.1 means answer can differ but should be generally correct\n",
    "# \n",
    "# Adjust these based on how strict you want the evaluation:\n",
    "# - 1.0 = Agent must be perfect\n",
    "# - 0.5 = Agent should be mostly correct\n",
    "# - 0.1 = Agent just needs to be in the ballpark\n",
    "\n",
    "print(\"âœ… Scoring criteria can be tuned based on your needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ab0f27",
   "metadata": {},
   "source": [
    "### How Evaluation Works: Step-by-Step\n",
    "\n",
    "1. **You define test scenarios** in `simple.test.json`\n",
    "   - Write the query\n",
    "   - Specify expected tools\n",
    "   - Provide reference answer\n",
    "\n",
    "2. **You set scoring criteria** in `test_config.json`\n",
    "   - How strict should tool matching be?\n",
    "   - How strict should answer matching be?\n",
    "\n",
    "3. **You run the evaluation:**\n",
    "   ```bash\n",
    "   uv run pytest eval/test_eval.py -v\n",
    "   ```\n",
    "\n",
    "4. **AgentEvaluator does the work:**\n",
    "   - Asks agent each query\n",
    "   - Records which tools agent used\n",
    "   - Compares agent's answer to reference\n",
    "   - Calculates scores\n",
    "   - Reports PASS/FAIL\n",
    "\n",
    "5. **You get detailed results:**\n",
    "   - âœ… Which scenarios passed\n",
    "   - âŒ Which scenarios failed\n",
    "   - ðŸ“Š Detailed scoring breakdown\n",
    "   - ðŸ” What the agent actually did vs. expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a5372",
   "metadata": {},
   "source": [
    "### Running Evaluation Tests\n",
    "\n",
    "```bash\n",
    "# Run the evaluation\n",
    "uv run pytest eval/test_eval.py -v\n",
    "\n",
    "# Run with detailed output\n",
    "uv run pytest eval/test_eval.py -vv -s\n",
    "\n",
    "# Run specific evaluation test\n",
    "uv run pytest eval/test_eval.py::test_eval_simple\n",
    "```\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "======================== test session starts =========================\n",
    "eval/test_eval.py::test_eval_simple \n",
    "\n",
    "Evaluating scenario 1/2: \"what data do you have?\"\n",
    "  âœ… Tool usage: PASS (no tools expected, none used)\n",
    "  âœ… Response quality: PASS (score: 0.85)\n",
    "\n",
    "Evaluating scenario 2/2: \"what countries are in test?\"\n",
    "  âœ… Tool usage: PASS (used call_db_agent as expected)\n",
    "  âœ… Response quality: PASS (score: 0.92)\n",
    "\n",
    "PASSED                                                          [100%]\n",
    "\n",
    "========================= 2 passed in 12.34s =========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6784f8",
   "metadata": {},
   "source": [
    "### Creating Your Own Evaluation Scenarios\n",
    "\n",
    "**Step 1:** Add a new scenario to `eval/eval_data/simple.test.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ef0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a churn evaluation scenario\n",
    "\n",
    "churn_scenario = {\n",
    "    \"query\": \"Show me customers with high churn risk\",\n",
    "    \n",
    "    \"expected_tool_use\": [\n",
    "        {\n",
    "            \"tool_name\": \"call_churn_agent\",\n",
    "            \"tool_input\": {\n",
    "                \"question\": \"Show me customers with high churn risk\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"expected_intermediate_agent_responses\": [],\n",
    "    \n",
    "    \"reference\": \"\"\"**High Churn Risk Customers:**\n",
    "\n",
    "I've identified customers with churn_metric > 1.5, indicating they are \n",
    "at high risk of churning. These customers show declining revenue trends \n",
    "and require immediate attention from the retention team.\n",
    "\n",
    "The analysis is based on comparing current revenue to previous revenue \n",
    "periods. High risk customers have significantly decreased their spending.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"âœ… This scenario tests if the agent:\")\n",
    "print(\"   1. Routes to churn_agent correctly\")\n",
    "print(\"   2. Understands 'high churn risk' classification\") \n",
    "print(\"   3. Provides helpful context in the answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28005200",
   "metadata": {},
   "source": [
    "**Step 2:** Add it to the JSON file:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"query\": \"what countries are in test?\",\n",
    "    \"expected_tool_use\": [...],\n",
    "    \"reference\": \"...\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"Show me customers with high churn risk\",\n",
    "    \"expected_tool_use\": [\n",
    "      {\n",
    "        \"tool_name\": \"call_churn_agent\",\n",
    "        \"tool_input\": {\n",
    "          \"question\": \"Show me customers with high churn risk\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"expected_intermediate_agent_responses\": [],\n",
    "    \"reference\": \"**High Churn Risk Customers:**\\n\\nI've identified customers...\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Step 3:** Run the evaluation to test your new scenario:\n",
    "```bash\n",
    "uv run pytest eval/test_eval.py -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60576b86",
   "metadata": {},
   "source": [
    "### When to Use Evaluation Tests\n",
    "\n",
    "| Use Case | Why |\n",
    "|----------|-----|\n",
    "| **Before releases** | Ensure agent quality hasn't degraded |\n",
    "| **After major changes** | Verify agent still handles key scenarios |\n",
    "| **Regression testing** | Prevent previously-fixed bugs from returning |\n",
    "| **CI/CD pipelines** | Automated quality gates |\n",
    "| **Comparing agent versions** | A/B test different prompts/configurations |\n",
    "\n",
    "### Evaluation vs Regular Tests\n",
    "\n",
    "| Feature | Regular Tests (`tests/`) | Evaluation Tests (`eval/`) |\n",
    "|---------|-------------------------|---------------------------|\n",
    "| **What** | Code works correctly | Agent is smart/helpful |\n",
    "| **Checks** | Functions return values | Agent uses right tools, gives good answers |\n",
    "| **Setup** | Write test methods | Write JSON scenarios |\n",
    "| **Scoring** | Pass/Fail | Graded scores (0.0-1.0) |\n",
    "| **Cost** | Free to $$$ | $$$ (many AI calls) |\n",
    "| **Speed** | Fast to slow | Slow (evaluates full conversations) |\n",
    "| **When** | Every commit | Weekly, before releases |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d99005",
   "metadata": {},
   "source": [
    "### Best Practices for Evaluation Tests\n",
    "\n",
    "âœ… **DO:**\n",
    "1. **Start with core user journeys** - Test the most common questions first\n",
    "2. **Use realistic queries** - Write questions like real users would ask\n",
    "3. **Keep reference answers flexible** - Agent can say things differently but still be correct\n",
    "4. **Update scenarios when fixing bugs** - Add failing case to prevent regression\n",
    "5. **Run before releases** - Make evaluation part of your release checklist\n",
    "\n",
    "âŒ **DON'T:**\n",
    "1. **Don't make scoring too strict** - `response_match_score: 1.0` requires exact match (unrealistic for AI)\n",
    "2. **Don't test everything** - Focus on critical paths, not edge cases\n",
    "3. **Don't ignore failures** - If eval fails, investigate why\n",
    "4. **Don't hardcode specific data** - Use general patterns that work as data changes\n",
    "5. **Don't run on every commit** - Too expensive; use CI/CD for weekly runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fd6d2",
   "metadata": {},
   "source": [
    "### Quick Reference: Evaluation Commands\n",
    "\n",
    "```bash\n",
    "# ========== EVALUATION TESTING ==========\n",
    "\n",
    "# Run evaluation tests\n",
    "uv run pytest eval/test_eval.py\n",
    "\n",
    "# Verbose output (shows each scenario)\n",
    "uv run pytest eval/test_eval.py -v\n",
    "\n",
    "# Very verbose (shows scoring details)\n",
    "uv run pytest eval/test_eval.py -vv -s\n",
    "\n",
    "# Run specific evaluation\n",
    "uv run pytest eval/test_eval.py::test_eval_simple\n",
    "\n",
    "# Run evaluation multiple times (for non-deterministic testing)\n",
    "# Edit eval/test_eval.py and change num_runs=1 to num_runs=3\n",
    "```\n",
    "\n",
    "### Example: Complete Evaluation Workflow\n",
    "\n",
    "1. **Create scenario file:** `eval/eval_data/churn_scenarios.json`\n",
    "2. **Define test in:** `eval/test_eval.py`\n",
    "   ```python\n",
    "   @pytest.mark.asyncio\n",
    "   async def test_eval_churn():\n",
    "       await AgentEvaluator.evaluate(\n",
    "           \"data_science\",\n",
    "           os.path.join(os.path.dirname(__file__), \"eval_data/churn_scenarios.json\"),\n",
    "           num_runs=1,\n",
    "       )\n",
    "   ```\n",
    "3. **Run:** `uv run pytest eval/test_eval.py::test_eval_churn -vv`\n",
    "4. **Review results** and iterate on agent prompts/tools\n",
    "5. **Repeat** until quality meets standards\n",
    "\n",
    "---\n",
    "\n",
    "ðŸŽ‰ **You now understand the complete testing system: `tests/` for code quality, `eval/` for agent intelligence!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351743c",
   "metadata": {},
   "source": [
    "## ðŸ› Debugging Failed Tests\n",
    "\n",
    "When tests fail, here's how to diagnose and fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222e90a",
   "metadata": {},
   "source": [
    "### Problem 1: \"Test Not Found\"\n",
    "\n",
    "```bash\n",
    "ERROR: module 'tests.test_churn_agent' has no attribute 'test_prompt'\n",
    "```\n",
    "\n",
    "**Cause:** Test names must start with `test_`\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Wrong âŒ\n",
    "async def check_prompt_has_rules(self):\n",
    "    ...\n",
    "\n",
    "# Right âœ…\n",
    "async def test_prompt_has_rules(self):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32120ed2",
   "metadata": {},
   "source": [
    "### Problem 2: \"ImportError: No module named 'data_science'\"\n",
    "\n",
    "```python\n",
    "ImportError: No module named 'data_science'\n",
    "```\n",
    "\n",
    "**Cause:** Running tests from wrong directory\n",
    "\n",
    "**Solution:** Always run from project root\n",
    "```bash\n",
    "# Wrong âŒ\n",
    "cd tests/\n",
    "python test_churn_agent.py\n",
    "\n",
    "# Right âœ…\n",
    "cd agent007/\n",
    "uv run python -m unittest tests/test_churn_agent.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a53555",
   "metadata": {},
   "source": [
    "### Problem 3: \"Environment Variables Not Set\"\n",
    "\n",
    "```python\n",
    "google.api_core.exceptions.NotFound: 404 ... project ID is empty\n",
    "```\n",
    "\n",
    "**Cause:** Missing `.env` file with credentials\n",
    "\n",
    "**Solution:** Create `agent007/.env` with:\n",
    "```bash\n",
    "BQ_DATA_PROJECT_ID=your-project-id\n",
    "BQ_DATASET_ID=agent007_outputs\n",
    "BQ_COMPUTE_PROJECT_ID=your-project-id\n",
    "GOOGLE_CLOUD_PROJECT=your-project-id\n",
    "ROOT_AGENT_MODEL=gemini-2.5-flash\n",
    "CHURN_AGENT_MODEL=gemini-2.5-flash\n",
    "OTEL_SDK_DISABLED=true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c09af2",
   "metadata": {},
   "source": [
    "### Problem 4: \"Test Takes Forever\"\n",
    "\n",
    "**Cause:** Network issues, wrong credentials, or infinite loops\n",
    "\n",
    "**Solution:** Add timeouts to tests\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def test_with_timeout(self):\n",
    "    # Fail if takes longer than 30 seconds\n",
    "    try:\n",
    "        async with asyncio.timeout(30):\n",
    "            response = self._run_agent(agent, query)\n",
    "    except asyncio.TimeoutError:\n",
    "        self.fail(\"Test took longer than 30 seconds\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e5532",
   "metadata": {},
   "source": [
    "### Debugging Tip: Add Print Statements\n",
    "\n",
    "Add debug prints to see what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Debugging with print statements\n",
    "\n",
    "async def test_with_debug_output():\n",
    "    \"\"\"Test with debug information.\"\"\"\n",
    "    prompt = churn_agent.instruction\n",
    "    \n",
    "    # DEBUG: Print what we're checking\n",
    "    print(f\"\\n=== DEBUG INFO ===\")\n",
    "    print(f\"Prompt length: {len(prompt)} characters\")\n",
    "    print(f\"First 200 chars: {prompt[:200]}...\")\n",
    "    print(f\"Contains 'HIGH CHURN RISK': {'HIGH CHURN RISK' in prompt}\")\n",
    "    print(f\"===================\\n\")\n",
    "    \n",
    "    # Now do the actual test\n",
    "    assert \"HIGH CHURN RISK\" in prompt\n",
    "    print(\"âœ… Test passed!\")\n",
    "\n",
    "# Run it\n",
    "await test_with_debug_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cb31b",
   "metadata": {},
   "source": [
    "## ðŸ“Š Advanced: Using pytest Markers\n",
    "\n",
    "Markers let you organize tests into groups and run them selectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9e554",
   "metadata": {},
   "source": [
    "### Step 1: Add Markers to Tests\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "class TestChurnAgent(unittest.IsolatedAsyncioTestCase):\n",
    "    \n",
    "    @pytest.mark.fast\n",
    "    @pytest.mark.unit\n",
    "    async def test_prompt_contains_rules(self):\n",
    "        \"\"\"Fast test - no AI calls.\"\"\"\n",
    "        prompt = churn_agent.instruction\n",
    "        assert \"HIGH CHURN RISK\" in prompt\n",
    "    \n",
    "    @pytest.mark.expensive\n",
    "    @pytest.mark.integration\n",
    "    async def test_query_real_data(self):\n",
    "        \"\"\"Expensive test - calls AI and database.\"\"\"\n",
    "        response = self._run_agent(churn_agent, \"Show high risk customers\")\n",
    "        assert response is not None\n",
    "    \n",
    "    @pytest.mark.slow\n",
    "    @pytest.mark.bqml\n",
    "    async def test_model_training(self):\n",
    "        \"\"\"Very slow test - trains ML model.\"\"\"\n",
    "        # ... model training code ...\n",
    "        pass\n",
    "```\n",
    "\n",
    "### Step 2: Run Specific Groups\n",
    "\n",
    "```bash\n",
    "# Run only fast tests\n",
    "uv run pytest tests/ -m fast\n",
    "\n",
    "# Run only expensive tests  \n",
    "uv run pytest tests/ -m expensive\n",
    "\n",
    "# Run everything EXCEPT expensive tests\n",
    "uv run pytest tests/ -m \"not expensive\"\n",
    "\n",
    "# Run integration tests only\n",
    "uv run pytest tests/ -m integration\n",
    "\n",
    "# Combine markers: fast OR unit tests\n",
    "uv run pytest tests/ -m \"fast or unit\"\n",
    "\n",
    "# Combine markers: integration AND not slow\n",
    "uv run pytest tests/ -m \"integration and not slow\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961467e3",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Best Practices Summary\n",
    "\n",
    "Follow these guidelines to write effective tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6671edfb",
   "metadata": {},
   "source": [
    "### âœ… DO:\n",
    "\n",
    "1. **Start with cheap tests**\n",
    "   ```python\n",
    "   # Test configuration first (free, fast)\n",
    "   async def test_agent_exists(self):\n",
    "       assert churn_agent is not None\n",
    "       assert churn_agent.name == \"churn_agent\"\n",
    "   ```\n",
    "\n",
    "2. **Give tests descriptive names**\n",
    "   ```python\n",
    "   # Good âœ…\n",
    "   async def test_churn_agent_identifies_high_risk_customers(self):\n",
    "       ...\n",
    "   \n",
    "   # Bad âŒ\n",
    "   async def test1(self):\n",
    "       ...\n",
    "   ```\n",
    "\n",
    "3. **Add docstrings to explain what's being tested**\n",
    "   ```python\n",
    "   async def test_lost_customer_classification(self):\n",
    "       \"\"\"\n",
    "       Verify that customers with previous_revenue > 0 and \n",
    "       current_revenue = 0 are classified as LOST.\n",
    "       \"\"\"\n",
    "       ...\n",
    "   ```\n",
    "\n",
    "4. **Print debug info for manual inspection**\n",
    "   ```python\n",
    "   response = self._run_agent(agent, query)\n",
    "   print(f\"\\n=== RESPONSE ===\\n{response}\\n\")\n",
    "   assert \"LOST\" in response\n",
    "   ```\n",
    "\n",
    "5. **Use markers to organize expensive tests**\n",
    "   ```python\n",
    "   @pytest.mark.expensive\n",
    "   @pytest.mark.slow\n",
    "   async def test_full_churn_analysis(self):\n",
    "       ...\n",
    "   ```\n",
    "\n",
    "6. **Test edge cases and error handling**\n",
    "   ```python\n",
    "   async def test_agent_handles_empty_data(self):\n",
    "       \"\"\"Test agent doesn't crash on empty dataset.\"\"\"\n",
    "       response = self._run_agent(agent, \"Show customers from year 3000\")\n",
    "       assert \"no data\" in response.lower() or \"not found\" in response.lower()\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444192ba",
   "metadata": {},
   "source": [
    "### âŒ DON'T:\n",
    "\n",
    "1. **Don't expect exact responses** (AI is non-deterministic)\n",
    "   ```python\n",
    "   # Bad âŒ - Response text will vary\n",
    "   self.assertEqual(response, \"There are 5 high-risk customers\")\n",
    "   \n",
    "   # Good âœ… - Check for key concepts\n",
    "   self.assertIn(\"high-risk\", response.lower())\n",
    "   self.assertIn(\"5\", response)\n",
    "   ```\n",
    "\n",
    "2. **Don't hardcode sensitive data**\n",
    "   ```python\n",
    "   # Bad âŒ\n",
    "   project_id = \"ap-eaip-exp-prod-07-8362\"\n",
    "   \n",
    "   # Good âœ…\n",
    "   import os\n",
    "   project_id = os.getenv(\"BQ_DATA_PROJECT_ID\")\n",
    "   ```\n",
    "\n",
    "3. **Don't write tests that depend on each other**\n",
    "   ```python\n",
    "   # Bad âŒ - test_2 depends on test_1 running first\n",
    "   async def test_1_create_data(self):\n",
    "       self.data = \"something\"\n",
    "   \n",
    "   async def test_2_use_data(self):\n",
    "       result = process(self.data)  # Fails if test_1 didn't run\n",
    "   \n",
    "   # Good âœ… - Each test is independent\n",
    "   async def test_1_create_data(self):\n",
    "       data = \"something\"\n",
    "       assert data is not None\n",
    "   \n",
    "   async def test_2_use_data(self):\n",
    "       data = \"something\"  # Create own data\n",
    "       result = process(data)\n",
    "       assert result is not None\n",
    "   ```\n",
    "\n",
    "4. **Don't skip cleaning up resources**\n",
    "   ```python\n",
    "   async def asyncTearDown(self):\n",
    "       \"\"\"Clean up after each test.\"\"\"\n",
    "       # Close connections, delete temp files, etc.\n",
    "       if hasattr(self, 'session'):\n",
    "           # Clean up session if needed\n",
    "           pass\n",
    "   ```\n",
    "\n",
    "5. **Don't test implementation details**\n",
    "   ```python\n",
    "   # Bad âŒ - Testing internal implementation\n",
    "   async def test_agent_uses_specific_function(self):\n",
    "       assert \"call_bigquery_api\" in str(agent.tools)\n",
    "   \n",
    "   # Good âœ… - Testing behavior/outcome\n",
    "   async def test_agent_can_query_database(self):\n",
    "       response = self._run_agent(agent, \"What's in the database?\")\n",
    "       assert response is not None\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eeb7e9",
   "metadata": {},
   "source": [
    "## ðŸš€ Quick Reference Cheat Sheet\n",
    "\n",
    "Your go-to reference for common testing commands and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbddde",
   "metadata": {},
   "source": [
    "### Running Tests\n",
    "\n",
    "```bash\n",
    "# ========== RUNNING TESTS ==========\n",
    "\n",
    "# Run all tests\n",
    "uv run pytest tests/\n",
    "\n",
    "# Run one file\n",
    "uv run pytest tests/test_churn_agent.py\n",
    "\n",
    "# Run one test\n",
    "uv run pytest tests/test_churn_agent.py::TestChurnAgent::test_prompt_contains_rules\n",
    "\n",
    "# Run with markers\n",
    "uv run pytest tests/ -m fast\n",
    "uv run pytest tests/ -m \"not expensive\"\n",
    "\n",
    "# Verbose output\n",
    "uv run pytest tests/ -v\n",
    "\n",
    "# Very verbose (detailed errors)\n",
    "uv run pytest tests/ -vv\n",
    "\n",
    "# Stop at first failure\n",
    "uv run pytest tests/ -x\n",
    "\n",
    "# Show print statements\n",
    "uv run pytest tests/ -s\n",
    "\n",
    "# Run with coverage\n",
    "uv run pytest tests/ --cov=data_science\n",
    "\n",
    "# Run specific test with unittest\n",
    "uv run python -m unittest tests.test_churn_agent.TestChurnAgent.test_prompt_contains_rules\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa906a5",
   "metadata": {},
   "source": [
    "### Common Assertions\n",
    "\n",
    "```python\n",
    "# ========== COMMON ASSERTIONS ==========\n",
    "\n",
    "# Equality\n",
    "self.assertEqual(a, b)           # a == b\n",
    "self.assertNotEqual(a, b)        # a != b\n",
    "\n",
    "# Boolean\n",
    "self.assertTrue(x)               # x is True\n",
    "self.assertFalse(x)              # x is False\n",
    "\n",
    "# Membership\n",
    "self.assertIn(item, list)        # item in list\n",
    "self.assertNotIn(item, list)     # item not in list\n",
    "\n",
    "# None checks\n",
    "self.assertIsNone(x)             # x is None\n",
    "self.assertIsNotNone(x)          # x is not None\n",
    "\n",
    "# Comparison\n",
    "self.assertGreater(a, b)         # a > b\n",
    "self.assertLess(a, b)            # a < b\n",
    "self.assertGreaterEqual(a, b)    # a >= b\n",
    "self.assertLessEqual(a, b)       # a <= b\n",
    "\n",
    "# Type checks\n",
    "self.assertIsInstance(obj, cls)  # isinstance(obj, cls)\n",
    "\n",
    "# String checks\n",
    "self.assertRegex(text, pattern)  # re.search(pattern, text)\n",
    "\n",
    "# Exception handling\n",
    "with self.assertRaises(ValueError):\n",
    "    function_that_raises_error()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36dd411",
   "metadata": {},
   "source": [
    "### Test Structure Template\n",
    "\n",
    "```python\n",
    "# ========== TEST STRUCTURE ==========\n",
    "\n",
    "import unittest\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.artifacts import InMemoryArtifactService\n",
    "from google.genai import types\n",
    "\n",
    "# Initialize services\n",
    "session_service = InMemorySessionService()\n",
    "artifact_service = InMemoryArtifactService()\n",
    "\n",
    "class TestMyAgent(unittest.IsolatedAsyncioTestCase):\n",
    "    \"\"\"Test cases for my agent.\"\"\"\n",
    "    \n",
    "    async def asyncSetUp(self):\n",
    "        \"\"\"Runs before each test.\"\"\"\n",
    "        self.session = await session_service.create_session(\n",
    "            app_name=\"MyApp\",\n",
    "            user_id=\"test_user\"\n",
    "        )\n",
    "        self.runner = Runner(\n",
    "            app_name=\"MyApp\",\n",
    "            agent=None,\n",
    "            artifact_service=artifact_service,\n",
    "            session_service=session_service\n",
    "        )\n",
    "        self.user_id = \"test_user\"\n",
    "        self.session_id = self.session.id\n",
    "    \n",
    "    def _run_agent(self, agent, query: str) -> str:\n",
    "        \"\"\"Helper to run agent with query.\"\"\"\n",
    "        self.runner.agent = agent\n",
    "        content = types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=query)]\n",
    "        )\n",
    "        events = list(self.runner.run(\n",
    "            user_id=self.user_id,\n",
    "            session_id=self.session_id,\n",
    "            new_message=content\n",
    "        ))\n",
    "        last_event = events[-1]\n",
    "        return \"\".join(\n",
    "            part.text \n",
    "            for part in last_event.content.parts \n",
    "            if part.text\n",
    "        )\n",
    "    \n",
    "    async def test_my_first_test(self):\n",
    "        \"\"\"Description of what this tests.\"\"\"\n",
    "        # Arrange: Set up test data\n",
    "        query = \"test query\"\n",
    "        \n",
    "        # Act: Run the code being tested\n",
    "        response = self._run_agent(my_agent, query)\n",
    "        \n",
    "        # Assert: Check the results\n",
    "        self.assertIsNotNone(response)\n",
    "        self.assertIn(\"expected\", response)\n",
    "    \n",
    "    async def asyncTearDown(self):\n",
    "        \"\"\"Runs after each test (cleanup).\"\"\"\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921c77f",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Required environment variables in `.env` file:\n",
    "\n",
    "```bash\n",
    "# ========== ENVIRONMENT SETUP ==========\n",
    "\n",
    "# Required for BigQuery tests\n",
    "BQ_DATA_PROJECT_ID=your-project-id\n",
    "BQ_DATASET_ID=agent007_outputs\n",
    "BQ_COMPUTE_PROJECT_ID=your-project-id\n",
    "GOOGLE_CLOUD_PROJECT=your-project-id\n",
    "\n",
    "# Required for LLM calls\n",
    "ROOT_AGENT_MODEL=gemini-2.5-flash\n",
    "BIGQUERY_AGENT_MODEL=gemini-2.5-flash\n",
    "CHURN_AGENT_MODEL=gemini-2.5-flash\n",
    "ANALYTICS_AGENT_MODEL=gemini-2.5-flash\n",
    "BQML_AGENT_MODEL=gemini-2.5-flash\n",
    "PLOT_AGENT_MODEL=gemini-2.5-flash\n",
    "\n",
    "# Optional: Disable telemetry in tests\n",
    "OTEL_SDK_DISABLED=true\n",
    "```\n",
    "\n",
    "Install test dependencies:\n",
    "```bash\n",
    "# Install dev dependencies including pytest\n",
    "uv sync --extra dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d6873a",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the Agent007 Testing Tutorial!\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "âœ… **What testing is** and why it matters  \n",
    "âœ… **How Agent007's testing system works**  \n",
    "âœ… **How to read and write tests**  \n",
    "âœ… **How to run tests** and interpret results  \n",
    "âœ… **How to debug** when tests fail  \n",
    "âœ… **Best practices** for effective testing  \n",
    "âœ… **Advanced techniques** like pytest markers  \n",
    "\n",
    "### Your Testing Journey:\n",
    "\n",
    "```\n",
    "Level 1: Beginner ðŸŒ±\n",
    "â”œâ”€ Understand what tests are\n",
    "â”œâ”€ Read existing tests\n",
    "â””â”€ Run tests with basic commands\n",
    "\n",
    "Level 2: Intermediate ðŸŒ¿\n",
    "â”œâ”€ Write simple configuration tests\n",
    "â”œâ”€ Write prompt validation tests\n",
    "â””â”€ Debug test failures\n",
    "\n",
    "Level 3: Advanced ðŸŒ³\n",
    "â”œâ”€ Write integration tests (with AI calls)\n",
    "â”œâ”€ Use pytest markers effectively\n",
    "â”œâ”€ Optimize test performance\n",
    "â””â”€ Write evaluation tests\n",
    "\n",
    "Level 4: Expert ðŸ†\n",
    "â”œâ”€ Design comprehensive test suites\n",
    "â”œâ”€ Balance cost vs coverage\n",
    "â”œâ”€ Implement CI/CD testing\n",
    "â””â”€ Mentor others in testing\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try running the existing tests:**\n",
    "   ```bash\n",
    "   cd /home/riga/agent007\n",
    "   uv run pytest tests/ -v\n",
    "   ```\n",
    "\n",
    "2. **Read through one test file** to understand the patterns:\n",
    "   ```bash\n",
    "   cat tests/test_churn_agent.py\n",
    "   ```\n",
    "\n",
    "3. **Write your own simple test** (start with configuration tests)\n",
    "\n",
    "4. **Gradually add more complex tests** as you gain confidence\n",
    "\n",
    "5. **Share your knowledge** with your team!\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Agent007 Test Files:** `tests/test_agents.py`, `tests/test_churn_agent.py`\n",
    "- **pytest Documentation:** https://docs.pytest.org/\n",
    "- **unittest Documentation:** https://docs.python.org/3/library/unittest.html\n",
    "- **ADK Documentation:** https://google.github.io/adk-docs/\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Testing! ðŸš€**\n",
    "\n",
    "*Remember: Good tests are like insurance - you're glad you have them when something goes wrong!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b87f60",
   "metadata": {},
   "source": [
    "## ðŸ“ Practice Exercise\n",
    "\n",
    "Try this hands-on exercise to test your understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3affaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRACTICE EXERCISE: Write a test that checks if the churn agent\n",
    "# has tools configured\n",
    "\n",
    "async def test_practice_churn_agent_has_tools():\n",
    "    \"\"\"\n",
    "    YOUR TASK: Write a test that checks if churn_agent has tools.\n",
    "    \n",
    "    Hints:\n",
    "    - Check if churn_agent.tools exists\n",
    "    - Check if it's not empty\n",
    "    - Check if it contains at least 1 tool\n",
    "    \n",
    "    Replace 'pass' with your test code below:\n",
    "    \"\"\"\n",
    "    # TODO: Write your test here\n",
    "    pass\n",
    "    \n",
    "    # SOLUTION (uncomment to see):\n",
    "    # assert hasattr(churn_agent, 'tools'), \"Agent should have tools attribute\"\n",
    "    # assert churn_agent.tools is not None, \"Tools should not be None\"\n",
    "    # assert len(churn_agent.tools) > 0, \"Agent should have at least one tool\"\n",
    "    # print(f\"âœ… Test passed! Agent has {len(churn_agent.tools)} tools\")\n",
    "\n",
    "# Try running your test!\n",
    "# await test_practice_churn_agent_has_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af604ef6",
   "metadata": {},
   "source": [
    "## ðŸ”§ Troubleshooting Common Issues\n",
    "\n",
    "### Issue: Notebook cells fail to import modules\n",
    "\n",
    "**Problem:** Can't import `data_science` modules in notebook\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Now imports should work\n",
    "from data_science.sub_agents.churn.agent import root_agent as churn_agent\n",
    "```\n",
    "\n",
    "### Issue: Environment variables not loaded\n",
    "\n",
    "**Problem:** Tests fail with \"project ID is empty\"\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from parent directory\n",
    "env_path = os.path.join(os.path.dirname(os.getcwd()), '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Verify it worked\n",
    "print(f\"Project ID: {os.getenv('BQ_DATA_PROJECT_ID')}\")\n",
    "```\n",
    "\n",
    "### Issue: Async functions not running in notebook\n",
    "\n",
    "**Problem:** `RuntimeError: This event loop is already running`\n",
    "\n",
    "**Solution:** In Jupyter notebooks, use `await` directly:\n",
    "```python\n",
    "# In regular Python files:\n",
    "asyncio.run(my_async_function())\n",
    "\n",
    "# In Jupyter notebooks:\n",
    "await my_async_function()  # Just use await directly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98211c67",
   "metadata": {},
   "source": [
    "| Test Type | Purpose | Cost | Speed | When to Use |\n",
    "|-----------|---------|------|-------|-------------|\n",
    "| **Unit (Configuration)** | Check if agents are set up correctly | $0 | < 1 sec | Every commit |\n",
    "| **Unit (Prompt)** | Check if instructions contain key phrases | $0 | < 1 sec | After changing prompts |\n",
    "| **Integration** | Actually run the agent with real AI | $0.01-0.10 | 5-30 sec | Before merging, important features |\n",
    "| **Evaluation** | Test multiple scenarios | $1-10 | Minutes | Weekly, before releases |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
