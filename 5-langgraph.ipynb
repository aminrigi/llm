{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dfbddee",
   "metadata": {},
   "source": [
    "# Langgraph\n",
    "\n",
    "### 0- Packages needed\n",
    "\n",
    "| **Package**    | **Purpose**                                                                   |\n",
    "| -------------- | ----------------------------------------------------------------------------- |\n",
    "| `langgraph`    | Core library for building dynamic graphs of function calls with state passing |\n",
    "| `langchain`    | Provides unified interfaces for LLMs, tools, chains, and pipelines            |\n",
    "| `transformers` | From Hugging Face – used to load and run local LLMs                           |\n",
    "| `accelerate`   | Helps efficiently load large models across CPU/GPU                            |\n",
    "| `einops`       | Required by many Hugging Face models for tensor reshaping                     |\n",
    "\n",
    "\n",
    "**Steps:**\n",
    "Build a simple LangGraph-based question-answering agent that:\n",
    "\n",
    "1- Understands a user’s question\n",
    "\n",
    "2- Decides if a math tool is needed\n",
    "\n",
    "3- Calls a calculator tool (if required)\n",
    "\n",
    "4- Generates a final response\n",
    "\n",
    "It will run entirely locally using a HuggingFace LLM (e.g., Mistral-7B-Instruct) with no external APIs.\n",
    "\n",
    "\n",
    "### 1- Loading a local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f244bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f007b5c09ebd4d45a3838282e10dea0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\projects\\gr\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rigia\\.cache\\huggingface\\hub\\models--tiiuae--falcon-rw-1b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7b682890964cf38fe5074d769ee84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a707b6582baf4f909ecc6805b1ca2a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16e631e28874854bf1cd8acd0e9a0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b39feb00b843bdbb6c65e3f209de74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992125b4b6154d979b3c2439a9972276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c43aef98054cc6a5275dda8fe0df8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3877985a6f05453e89e854dc917244a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6395f1e38b744251b52d8bc469554770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_29612\\2112411347.py:17: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  local_llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model (replace with your local path if needed)\n",
    "model_id = \"tiiuae/falcon-rw-1b\" # simple and fast model for testing\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create a text-generation pipeline and wrap it with LangChain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50177f4",
   "metadata": {},
   "source": [
    "### 2- Define the agent state\n",
    "\n",
    "We define a shared state to pass information between LangGraph nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9345836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    steps: List[str]\n",
    "    tool_input: str\n",
    "    tool_output: str\n",
    "    final_answer: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21681ffa",
   "metadata": {},
   "source": [
    "| **Field**      | **Type**    | **Description**                                                           | **When It's Set / Updated**                                              | **Used By**                           |\n",
    "| -------------- | ----------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ------------------------------------- |\n",
    "| `input`        | `str`       | The original user question or prompt                                      | Initially set when invoking the graph                                    | All nodes (decision, synthesis, etc.) |\n",
    "| `steps`        | `List[str]` | (Optional) Logs intermediate steps, decisions, or tool calls              | Can be appended to at each step for trace/debugging (optional use)       | Optional: for tracing/debug/debugging |\n",
    "| `tool_input`   | `str`       | The expression or command extracted from the input to be passed to a tool | Set in `extract_tool_input` node (if tool is used)                       | `call_tool`                           |\n",
    "| `tool_output`  | `str`       | The result returned from the tool (e.g., evaluated math expression)       | Set in `call_tool` node                                                  | `synthesize`                          |\n",
    "| `final_answer` | `str`       | The final response generated by the LLM agent                             | Set in `synthesize` node (based on `input` and optionally `tool_output`) | Final output returned to the user     |\n",
    "\n",
    "\n",
    "**Defining a calculator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5592c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator(expression: str) -> str:\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except:\n",
    "        return \"Could not evaluate expression.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d73b8e",
   "metadata": {},
   "source": [
    "### 3- Create Nodes (functions in the graph)\n",
    "\n",
    "Each node is a function and takes/returns agentState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23c0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1 handle_input\n",
    "def handle_input(state: AgentState) -> AgentState:\n",
    "    print(\"📥 User input:\", state[\"input\"])\n",
    "    return state\n",
    "\n",
    "\n",
    "# 3-2 decide_tool\n",
    "def decide_tool(state: AgentState) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant. Does this question require a math calculation?\n",
    "\n",
    "    Question: {state['input']}\n",
    "\n",
    "    Respond with 'use_tool' or 'skip_tool'.\n",
    "    \"\"\"\n",
    "    response = local_llm(prompt)\n",
    "    decision = \"use_tool\" if \"use_tool\" in response.lower() else \"skip_tool\"\n",
    "    print(\"🧭 Decision:\", decision)\n",
    "    return decision\n",
    "\n",
    "\n",
    "# 3-3 extract_tool_input\n",
    "def extract_tool_input(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"\n",
    "    Extract the math expression from this question: \"{state['input']}\".\n",
    "    Just return the expression, nothing else.\n",
    "    \"\"\"\n",
    "    expression = local_llm(prompt).strip()\n",
    "    state[\"tool_input\"] = expression\n",
    "    return state\n",
    "\n",
    "\n",
    "# 3-4 call_tool\n",
    "def call_tool(state: AgentState) -> AgentState:\n",
    "    expr = state.get(\"tool_input\", \"\")\n",
    "    state[\"tool_output\"] = calculator(expr)\n",
    "    return state\n",
    "\n",
    "# 3-5 finalize_answer\n",
    "\n",
    "def synthesize(state: AgentState) -> AgentState:\n",
    "    if state.get(\"tool_output\"):\n",
    "        prompt = f\"\"\"\n",
    "        The user asked: \"{state['input']}\".\n",
    "        You used a tool and got: {state['tool_output']}.\n",
    "        Provide the final answer.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"Answer the question: {state['input']}\"\n",
    "    \n",
    "    final = local_llm(prompt).strip()\n",
    "    state[\"final_answer\"] = final\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef963a",
   "metadata": {},
   "source": [
    "### 4- Building LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cecbc6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   +-----------+          \n",
      "                   | __start__ |          \n",
      "                   +-----------+          \n",
      "                          *               \n",
      "                          *               \n",
      "                          *               \n",
      "                 +---------------+        \n",
      "                 | input_handler |        \n",
      "                 +---------------+        \n",
      "                 ...            ...       \n",
      "               ..                  ..     \n",
      "             ..                      ..   \n",
      "+--------------------+                 .. \n",
      "| extract_tool_input |                  . \n",
      "+--------------------+                  . \n",
      "           *                            . \n",
      "           *                            . \n",
      "           *                            . \n",
      "    +-----------+                      .. \n",
      "    | call_tool |                    ..   \n",
      "    +-----------+                  ..     \n",
      "                 ***            ...       \n",
      "                    **        ..          \n",
      "                      **    ..            \n",
      "                  +------------+          \n",
      "                  | synthesize |          \n",
      "                  +------------+          \n",
      "                          *               \n",
      "                          *               \n",
      "                          *               \n",
      "                    +---------+           \n",
      "                    | __end__ |           \n",
      "                    +---------+           \n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"input_handler\", handle_input)\n",
    "graph.add_node(\"extract_tool_input\", extract_tool_input)\n",
    "graph.add_node(\"call_tool\", call_tool)\n",
    "graph.add_node(\"synthesize\", synthesize)\n",
    "\n",
    "# Add the entrypoint\n",
    "graph.add_edge(START, \"input_handler\")\n",
    "\n",
    "# Add conditional routing logic\n",
    "graph.add_conditional_edges(\n",
    "    \"input_handler\",\n",
    "    decide_tool,\n",
    "    {\n",
    "        \"use_tool\": \"extract_tool_input\",\n",
    "        \"skip_tool\": \"synthesize\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Normal transitions\n",
    "graph.add_edge(\"extract_tool_input\", \"call_tool\")\n",
    "graph.add_edge(\"call_tool\", \"synthesize\")\n",
    "graph.add_edge(\"synthesize\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent_executor = graph.compile()\n",
    "print(agent_executor.get_graph().draw_ascii())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1992a",
   "metadata": {},
   "source": [
    "### 5- Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a51701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_29612\\2316854134.py:16: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = local_llm(prompt)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 User input: What is 12 * (5 + 3)?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧭 Decision: use_tool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Answer: The user asked: \"What is 12 * (5 + 3)?\".\n",
      "        You used a tool and got: Could not evaluate expression..\n",
      "        Provide the final answer.\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"input\": \"What is 12 * (5 + 3)?\",\n",
    "    \"steps\": [],\n",
    "    \"tool_input\": \"\",\n",
    "    \"tool_output\": \"\",\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "final_state = agent_executor.invoke(initial_state)\n",
    "print(\"\\n✅ Final Answer:\", final_state[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87c092",
   "metadata": {},
   "source": [
    "## Using LangGraph with multiple knowledgebases\n",
    "\n",
    "Using LangGraph to orchestrate different RAG systems/databases for different domains is a very good idea, especially when:\n",
    "\n",
    "| **Reason**                                      | **Explanation**                                                                                                                          |\n",
    "| ----------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Modular routing**                             | LangGraph allows you to dynamically choose which retriever/knowledge base to use based on the user input or detected domain.             |\n",
    "| **Separation of concerns**                      | Each RAG system can have its own chunking logic, embeddings, and vector store tuned for its domain (e.g., legal, medical, product info). |\n",
    "| **Scalable and maintainable**                   | Easier to maintain or swap out domain-specific RAG pipelines without affecting others.                                                   |\n",
    "| **Tool selection logic is native to LangGraph** | You can use conditional routing (`add_conditional_edges`) to switch between domain-specific RAG branches.                                |\n",
    "| **Reusability and extension**                   | You can add fallback paths (e.g., general RAG or LLM-only) if domain-specific RAG yields low-confidence results.                         |\n",
    "\n",
    "\n",
    "User Input  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;↓  \n",
    "detect_domain  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── medical → medical_retrieve → medical_synthesize  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── legal → legal_retrieve → legal_synthesize  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── ecommerce → product_retrieve → product_synthesize  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;└── unknown → fallback_synthesize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf479ee3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5bced35",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
