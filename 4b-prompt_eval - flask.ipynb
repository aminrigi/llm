{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000b598e",
   "metadata": {},
   "source": [
    "Set up and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40af9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbb977",
   "metadata": {},
   "source": [
    "#### 0- Load VecDB and transforming into a retriver interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7259c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_29264\\45222999.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_29264\\45222999.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding)\n",
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_29264\\45222999.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198607ac",
   "metadata": {},
   "source": [
    "### 1- Define Prompt Logic\n",
    "\n",
    "- temperature: Controls randomness in generation. Lower values make the output more deterministic, while higher values increase diversity.\n",
    "- top_k: Limits the number of highest-probability tokens considered during generation.\n",
    "- top_p: Implements nucleus sampling, where only tokens with cumulative probability p are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc6cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_29264\\190748452.py:23: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_29264\\190748452.py:23: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"Use the following context to answer the question. Be concise and accurate.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "llm_pipeline = pipeline(\"text2text-generation\", \n",
    "                        model=AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\"), \n",
    "                        tokenizer=AutoTokenizer.from_pretrained(\"google/flan-t5-base\"),\n",
    "                        max_new_tokens=100,\n",
    "                        )\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228509e",
   "metadata": {},
   "source": [
    "| Parameter            | Range         | Purpose/Effect                                                                 | Recommended Values by Use Case         |\n",
    "|----------------------|---------------|--------------------------------------------------------------------------------|----------------------------------------|\n",
    "| **Temperature**      | 0.0 – 1.0     | Controls randomness. Lower = more deterministic; higher = more creative.      | - Deterministic: `0.2`<br>- Balanced: `0.5`<br>- Creative: `0.9` |\n",
    "| **Top-k**            | 1 – 50        | Limits sampling to top-k tokens. Lower = focused; higher = diverse.           | - Deterministic: `5`<br>- Balanced: `20`<br>- Creative: `40`     |\n",
    "| **Top-p**            | 0.0 – 1.0     | Nucleus sampling. Lower = deterministic; higher = creative.                   | - Deterministic: `0.3`<br>- Balanced: `0.6`<br>- Creative: `0.9` |\n",
    "| **Repetition Penalty** | 1.0 – 2.0   | Penalizes repeated tokens. Higher = less repetition.                          | - Deterministic: `1.0`<br>- Balanced: `1.2`<br>- Creative: `1.5` |\n",
    "\n",
    "\n",
    "### 2- Interface and prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f44b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1634 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is best approach to deal with missing val...</td>\n",
       "      <td>Forward fill.</td>\n",
       "      <td>One\\tof\\tthe\\tsimplest\\tways\\tto\\tfill\\tin\\tmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features are important for time series?</td>\n",
       "      <td>(4).</td>\n",
       "      <td>Chapter\\t8.\\t Generating\\tand\\tSelecting\\nFeat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of time series analysis?</td>\n",
       "      <td>Extracting meaningful summary and statistical ...</td>\n",
       "      <td>\\ta\\tsurfeit\\tof\\nfeatures\\tthan\\tit\\tis\\tto\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is best approach to deal with missing val...   \n",
       "1       What features are important for time series?   \n",
       "2       What is the purpose of time series analysis?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                      Forward fill.   \n",
       "1                                               (4).   \n",
       "2  Extracting meaningful summary and statistical ...   \n",
       "\n",
       "                                             context  \n",
       "0  One\\tof\\tthe\\tsimplest\\tways\\tto\\tfill\\tin\\tmi...  \n",
       "1  Chapter\\t8.\\t Generating\\tand\\tSelecting\\nFeat...  \n",
       "2  \\ta\\tsurfeit\\tof\\nfeatures\\tthan\\tit\\tis\\tto\\t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"What is best approach to deal with missing values?\",\n",
    "    \"What features are important for time series?\",\n",
    "    \"What is the purpose of time series analysis?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for q in questions:\n",
    "    res = qa_chain.invoke({\"query\": q})  # Updated to use invoke method\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": res['result'],\n",
    "        \"context\": res['source_documents'][0].page_content if res['source_documents'] else \"N/A\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2753e",
   "metadata": {},
   "source": [
    "### 3- Evaluation\n",
    "\n",
    "\n",
    "| **Metric**              | **Measures**                             | **Strengths**                                                                | **Limitations**                                                       | **Scoring Range**  | **Ideal Use Cases**                      | **Implementation Notes**                                     |\n",
    "| ----------------------- | ---------------------------------------- | ---------------------------------------------------------------------------- | --------------------------------------------------------------------- | ------------------ | ---------------------------------------- | ------------------------------------------------------------ |\n",
    "| **String Similarity**   | Character-level sequence overlap         | Fast, simple, interpretable                                                  | Sensitive to word order; ignores semantics                            | 0 to 1             | Quick sanity checks, small tweaks        | Uses `difflib.SequenceMatcher` in Python                     |\n",
    "| **BLEU**                | N-gram precision (1–4 grams)             | Widely used in machine translation, captures short patterns                  | Favors shorter outputs; penalizes valid rephrasings                   | 0 to 1             | Factual Q\\&A, summaries with gold labels | Use Hugging Face `evaluate` library                          |\n",
    "| **ROUGE-L**             | Longest common subsequence               | Good for content overlap and partial matches                                 | Sensitive to formatting and verbosity                                 | 0 to 1             | Summarization, Q\\&A                      | `rougeL` is best suited for factual or summary comparison    |\n",
    "| **Semantic Similarity** | Cosine similarity of sentence embeddings | Captures paraphrasing, semantically close responses                          | Can be tricked by verbose, vague, or generic answers                  | –1 to 1            | Open-ended answers, QA with synonyms     | Use `sentence-transformers` with `cos_sim`                   |\n",
    "| **LLM-as-a-Judge**      | Scored by an LLM based on custom prompt  | Human-like judgment; considers coherence, completeness, and factual accuracy | Subjective; expensive (API calls); inconsistent without prompt tuning | 0 to 5 (or custom) | High-stakes eval, human-like ranking     | Requires well-crafted prompts; best for few-shot comparisons |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**3.1- String similarity: Simple similarity match with ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c77e47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is best approach to deal with missing val...</td>\n",
       "      <td>Forward fill.</td>\n",
       "      <td>A framework for building LLM applications.</td>\n",
       "      <td>0.290909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features are important for time series?</td>\n",
       "      <td>(4).</td>\n",
       "      <td>A method to retrieve documents based on vector...</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of time series analysis?</td>\n",
       "      <td>Extracting meaningful summary and statistical ...</td>\n",
       "      <td>To split large text into manageable and semant...</td>\n",
       "      <td>0.080268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is best approach to deal with missing val...   \n",
       "1       What features are important for time series?   \n",
       "2       What is the purpose of time series analysis?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                      Forward fill.   \n",
       "1                                               (4).   \n",
       "2  Extracting meaningful summary and statistical ...   \n",
       "\n",
       "                                    reference_answer  similarity  \n",
       "0         A framework for building LLM applications.    0.290909  \n",
       "1  A method to retrieve documents based on vector...    0.032258  \n",
       "2  To split large text into manageable and semant...    0.080268  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can optionally add ground truth comparison if available\n",
    "df[\"reference_answer\"] = [\n",
    "    \"A framework for building LLM applications.\",\n",
    "    \"A method to retrieve documents based on vector similarity.\",\n",
    "    \"To split large text into manageable and semantically meaningful pieces.\"\n",
    "]\n",
    "\n",
    "# Simple string similarity score\n",
    "from difflib import SequenceMatcher\n",
    "df[\"similarity\"] = df.apply(lambda row: SequenceMatcher(None, row[\"answer\"], row[\"reference_answer\"]).ratio(), axis=1)\n",
    "df[[\"question\", \"answer\", \"reference_answer\", \"similarity\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806dc3f",
   "metadata": {},
   "source": [
    "**3.2- BLEU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45af71bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043850630a5640da8d94fcc3f738f9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df73841603649f9bad5df2c4bffdb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb882a52c6f54278b25901d2100ab952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is best approach to deal with missing val...</td>\n",
       "      <td>Forward fill.</td>\n",
       "      <td>A framework for building LLM applications.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features are important for time series?</td>\n",
       "      <td>(4).</td>\n",
       "      <td>A method to retrieve documents based on vector...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of time series analysis?</td>\n",
       "      <td>Extracting meaningful summary and statistical ...</td>\n",
       "      <td>To split large text into manageable and semant...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is best approach to deal with missing val...   \n",
       "1       What features are important for time series?   \n",
       "2       What is the purpose of time series analysis?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                      Forward fill.   \n",
       "1                                               (4).   \n",
       "2  Extracting meaningful summary and statistical ...   \n",
       "\n",
       "                                    reference_answer  bleu  \n",
       "0         A framework for building LLM applications.   0.0  \n",
       "1  A method to retrieve documents based on vector...   0.0  \n",
       "2  To split large text into manageable and semant...   0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "df[\"bleu\"] = df.apply(lambda row: bleu.compute(predictions=[row[\"answer\"]], references=[[row[\"reference_answer\"]]])[\"bleu\"], axis=1)\n",
    "\n",
    "df[[\"question\", \"answer\", \"reference_answer\", \"bleu\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfc9e9",
   "metadata": {},
   "source": [
    "**3.3- Rouge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c94ff9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is best approach to deal with missing val...</td>\n",
       "      <td>Forward fill.</td>\n",
       "      <td>A framework for building LLM applications.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features are important for time series?</td>\n",
       "      <td>(4).</td>\n",
       "      <td>A method to retrieve documents based on vector...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of time series analysis?</td>\n",
       "      <td>Extracting meaningful summary and statistical ...</td>\n",
       "      <td>To split large text into manageable and semant...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is best approach to deal with missing val...   \n",
       "1       What features are important for time series?   \n",
       "2       What is the purpose of time series analysis?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                      Forward fill.   \n",
       "1                                               (4).   \n",
       "2  Extracting meaningful summary and statistical ...   \n",
       "\n",
       "                                    reference_answer  bleu    rougeL  \n",
       "0         A framework for building LLM applications.   0.0  0.000000  \n",
       "1  A method to retrieve documents based on vector...   0.0  0.000000  \n",
       "2  To split large text into manageable and semant...   0.0  0.043956  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "df[\"rougeL\"] = df.apply(lambda row: rouge.compute(predictions=[row[\"answer\"]], references=[row[\"reference_answer\"]])[\"rougeL\"], axis=1)\n",
    "df[[\"question\", \"answer\", \"reference_answer\", \"bleu\", \"rougeL\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466c6e5",
   "metadata": {},
   "source": [
    "**3.4- Semantic Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12ef863d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is best approach to deal with missing val...</td>\n",
       "      <td>Forward fill.</td>\n",
       "      <td>A framework for building LLM applications.</td>\n",
       "      <td>0.056915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features are important for time series?</td>\n",
       "      <td>(4).</td>\n",
       "      <td>A method to retrieve documents based on vector...</td>\n",
       "      <td>-0.025989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of time series analysis?</td>\n",
       "      <td>Extracting meaningful summary and statistical ...</td>\n",
       "      <td>To split large text into manageable and semant...</td>\n",
       "      <td>0.148686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is best approach to deal with missing val...   \n",
       "1       What features are important for time series?   \n",
       "2       What is the purpose of time series analysis?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                      Forward fill.   \n",
       "1                                               (4).   \n",
       "2  Extracting meaningful summary and statistical ...   \n",
       "\n",
       "                                    reference_answer  semantic_similarity  \n",
       "0         A framework for building LLM applications.             0.056915  \n",
       "1  A method to retrieve documents based on vector...            -0.025989  \n",
       "2  To split large text into manageable and semant...             0.148686  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "df[\"semantic_similarity\"] = df.apply(lambda row: util.cos_sim(\n",
    "    model.encode(row[\"answer\"], convert_to_tensor=True),\n",
    "    model.encode(row[\"reference_answer\"], convert_to_tensor=True)\n",
    ").item(), axis=1)\n",
    "\n",
    "df[[\"question\", \"answer\", \"reference_answer\", \"semantic_similarity\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb10e2d",
   "metadata": {},
   "source": [
    "**3.5- LLM as a judge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8700f9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>llm_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is best approach to deal with missing val...</td>\n",
       "      <td>Forward fill.</td>\n",
       "      <td>A framework for building LLM applications.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features are important for time series?</td>\n",
       "      <td>(4).</td>\n",
       "      <td>A method to retrieve documents based on vector...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of time series analysis?</td>\n",
       "      <td>Extracting meaningful summary and statistical ...</td>\n",
       "      <td>To split large text into manageable and semant...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is best approach to deal with missing val...   \n",
       "1       What features are important for time series?   \n",
       "2       What is the purpose of time series analysis?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                      Forward fill.   \n",
       "1                                               (4).   \n",
       "2  Extracting meaningful summary and statistical ...   \n",
       "\n",
       "                                    reference_answer llm_score  \n",
       "0         A framework for building LLM applications.         0  \n",
       "1  A method to retrieve documents based on vector...         4  \n",
       "2  To split large text into manageable and semant...         3  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grading prompt\n",
    "\n",
    "grading_prompt = \"\"\"\n",
    "You are a strict evaluator. Given a question, a reference answer, and a model-generated answer, score the model's answer from 0 to 5 based on how accurate and complete it is.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Reference Answer: {reference}\n",
    "Model Answer: {model_answer}\n",
    "\n",
    "Score (0 to 5):\n",
    "\"\"\"\n",
    "\n",
    "def llm_grade(row):\n",
    "    input_text = grading_prompt.format(\n",
    "        question=row[\"question\"],\n",
    "        reference=row[\"reference_answer\"],\n",
    "        model_answer=row[\"answer\"]\n",
    "    )\n",
    "    return llm_pipeline(input_text)[0][\"generated_text\"]\n",
    "\n",
    "df[\"llm_score\"] = df.apply(llm_grade, axis=1)\n",
    "df[[\"question\", \"answer\", \"reference_answer\", \"llm_score\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd33772",
   "metadata": {},
   "source": [
    "\n",
    "# 🧠 Productionizing a RAG System (One-Pager)\n",
    "\n",
    "## 🧩 Architecture Overview\n",
    "```\n",
    "1. Document Ingestion\n",
    "        ↓\n",
    "2. Chunking & Embedding (sentence-transformers)\n",
    "        ↓\n",
    "3. Store in Vector DB (Qdrant / Weaviate)\n",
    "        ↓\n",
    "4. Query Handling + Retrieval (LangChain Retriever)\n",
    "        ↓\n",
    "5. Prompt Injection + LLM Inference (flan-t5 / mistral + vLLM)\n",
    "        ↓\n",
    "6. Evaluation (BLEU / ROUGE / Semantic Similarity)\n",
    "        ↓\n",
    "7. Feedback Loop + Monitoring\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Key Components & Tools\n",
    "\n",
    "| Component           | Description                                                  | Recommended Tech                                      |\n",
    "|--------------------|--------------------------------------------------------------|-------------------------------------------------------|\n",
    "| **Chunking**        | Split documents into semantic chunks                         | LangChain, Unstructured                              |\n",
    "| **Embedding**       | Encode chunks into dense vectors                             | `BAAI/bge-base`, `MiniLM`, `instructor-xl`           |\n",
    "| **Vector Store**    | Store & retrieve embeddings                                  | Qdrant, Weaviate, Chroma (dev), FAISS                |\n",
    "| **Retriever**       | Top-k / MMR retrieval of context                             | LangChain, Haystack                                  |\n",
    "| **Prompting**       | Custom templates to guide LLM output                         | LangChain PromptTemplate                             |\n",
    "| **LLM Inference**   | Generate answer from context + query                         | flan-t5, mistral, LLaMA2 via vLLM or TGI             |\n",
    "| **Evaluation**      | Assess answer quality                                        | BLEU, ROUGE, Semantic Similarity, LLM-as-a-Judge     |\n",
    "| **Serving API**     | Handle external queries                                      | FastAPI, Flask, LangServe                            |\n",
    "| **Deployment**      | Orchestrate and scale system                                 | Docker, K8s, Terraform, GitHub Actions               |\n",
    "| **Monitoring**      | Track performance & detect drift                             | Prometheus, Grafana, LangSmith, Evidently            |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Feedback Loop\n",
    "\n",
    "- Capture user feedback (thumbs up/down)\n",
    "- Log question + context + answer + feedback\n",
    "- Fine-tune embedding model or LLM based on evaluation drift\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Deployment Flow (CI/CD)\n",
    "\n",
    "1. Develop in Jupyter or VSCode (unit tested)\n",
    "2. Containerize with Docker\n",
    "3. Deploy via GitHub Actions or Terraform to K8s / SageMaker\n",
    "4. Monitor endpoints and vector drift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0ef02",
   "metadata": {},
   "source": [
    "### 4- Flask Interface for RAG Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ba58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Flask RAG Interface is running!\n",
      "📱 Web Interface: http://127.0.0.1:5000\n",
      "🔗 API Endpoint: http://127.0.0.1:5000/rag_query\n",
      "\n",
      "📝 Example API usage:\n",
      "curl -X POST http://127.0.0.1:5000/rag_query -H 'Content-Type: application/json' -d '{\"question\": \"What is the best approach to deal with missing values?\"}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [10/Jul/2025 15:48:43] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jul/2025 15:48:44] \"127.0.0.1 - - [10/Jul/2025 15:48:44] \"GET /favicon.ico HTTP/1.1GET /favicon.ico HTTP/1.1\" 404 -\n",
      "\" 404 -\n",
      "127.0.0.1 - - [10/Jul/2025 15:48:46] \"127.0.0.1 - - [10/Jul/2025 15:48:46] \"GET /favicon.ico HTTP/1.1GET /favicon.ico HTTP/1.1\" 404 -\n",
      "\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "import threading\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# HTML template for the web interface\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>RAG System Testing Interface</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }\n",
    "        .container { background: #f5f5f5; padding: 20px; border-radius: 8px; margin: 10px 0; }\n",
    "        input[type=\"text\"] { width: 70%; padding: 10px; font-size: 16px; }\n",
    "        button { padding: 10px 20px; font-size: 16px; background: #007bff; color: white; border: none; border-radius: 4px; cursor: pointer; }\n",
    "        button:hover { background: #0056b3; }\n",
    "        .result { background: white; padding: 15px; margin: 10px 0; border-left: 4px solid #007bff; }\n",
    "        .context { background: #e9ecef; padding: 10px; margin: 10px 0; border-radius: 4px; font-size: 14px; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>🧠 RAG System Testing Interface</h1>\n",
    "    \n",
    "    <div class=\"container\">\n",
    "        <h3>Ask a Question</h3>\n",
    "        <input type=\"text\" id=\"question\" placeholder=\"Enter your question here...\" value=\"What is the best approach to deal with missing values?\">\n",
    "        <button onclick=\"askQuestion()\">Ask RAG System</button>\n",
    "    </div>\n",
    "    \n",
    "    <div id=\"results\"></div>\n",
    "\n",
    "    <script>\n",
    "        async function askQuestion() {\n",
    "            const question = document.getElementById('question').value;\n",
    "            if (!question.trim()) return;\n",
    "            \n",
    "            document.getElementById('results').innerHTML = '<p>Processing...</p>';\n",
    "            \n",
    "            try {\n",
    "                const response = await fetch('/rag_query', {\n",
    "                    method: 'POST',\n",
    "                    headers: {'Content-Type': 'application/json'},\n",
    "                    body: JSON.stringify({question: question})\n",
    "                });\n",
    "                \n",
    "                const data = await response.json();\n",
    "                \n",
    "                if (response.ok) {\n",
    "                    document.getElementById('results').innerHTML = `\n",
    "                        <div class=\"result\">\n",
    "                            <h4>Question:</h4>\n",
    "                            <p>${data.question}</p>\n",
    "                            <h4>Answer:</h4>\n",
    "                            <p>${data.answer}</p>\n",
    "                            <h4>Context Used:</h4>\n",
    "                            <div class=\"context\">${data.context}</div>\n",
    "                        </div>\n",
    "                    `;\n",
    "                } else {\n",
    "                    document.getElementById('results').innerHTML = `<p style=\"color: red;\">Error: ${data.error}</p>`;\n",
    "                }\n",
    "            } catch (error) {\n",
    "                document.getElementById('results').innerHTML = `<p style=\"color: red;\">Error: ${error.message}</p>`;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Allow Enter key to submit\n",
    "        document.getElementById('question').addEventListener('keypress', function(e) {\n",
    "            if (e.key === 'Enter') askQuestion();\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template_string(HTML_TEMPLATE)\n",
    "\n",
    "@app.route('/rag_query', methods=['POST'])\n",
    "def rag_query():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        question = data.get('question', '').strip()\n",
    "        \n",
    "        if not question:\n",
    "            return jsonify({'error': 'No question provided'}), 400\n",
    "        \n",
    "        # Query the RAG system\n",
    "        res = qa_chain.invoke({\"query\": question})\n",
    "        answer = res['result']\n",
    "        context = res['source_documents'][0].page_content if res['source_documents'] else \"No context retrieved\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'context': context\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "def run_flask():\n",
    "    app.run(host='127.0.0.1', port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "# Start Flask in a background thread\n",
    "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "flask_thread.start()\n",
    "\n",
    "# Give the server a moment to start\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"🚀 Flask RAG Interface is running!\")\n",
    "print(\"📱 Web Interface: http://127.0.0.1:5000\")\n",
    "print(\"🔗 API Endpoint: http://127.0.0.1:5000/rag_query\")\n",
    "print(\"\\n📝 Example API usage:\")\n",
    "print(\"curl -X POST http://127.0.0.1:5000/rag_query -H 'Content-Type: application/json' -d '{\\\"question\\\": \\\"What is the best approach to deal with missing values?\\\"}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4812d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rigia\\AppData\\Local\\Temp\\ipykernel_29264\\628756584.py:9: DeprecationWarning: connections() is deprecated and will be removed; use net_connections() instead\n",
      "  for conn in proc.connections():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 Stopping process 0 using port 5000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "try:\n",
    "    # Try to find and stop any process using port 5000\n",
    "    import psutil\n",
    "    for proc in psutil.process_iter(['pid', 'name']):\n",
    "        try:\n",
    "            for conn in proc.connections():\n",
    "                if conn.laddr.port == 5000:\n",
    "                    print(f\"🛑 Stopping process {proc.pid} using port 5000\")\n",
    "                    proc.terminate()\n",
    "                    break\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            pass\n",
    "    print(\"🛑 Flask server stopped.\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ psutil not available. To stop Flask server, restart the kernel.\")\n",
    "    print(\"💡 The Flask server will stop automatically when you restart the kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Flask API programmatically\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def test_rag_api(question):\n",
    "    \"\"\"Test function to query the RAG API\"\"\"\n",
    "    try:\n",
    "        url = \"http://127.0.0.1:5000/rag_query\"\n",
    "        payload = {\"question\": question}\n",
    "        \n",
    "        response = requests.post(url, json=payload, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"✅ Question: {result['question']}\")\n",
    "            print(f\"🤖 Answer: {result['answer']}\")\n",
    "            print(f\"📄 Context: {result['context'][:200]}...\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"❌ Error {response.status_code}: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Connection error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a sample question\n",
    "print(\"🧪 Testing RAG API...\")\n",
    "test_question = \"What features are important for time series?\"\n",
    "test_rag_api(test_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
